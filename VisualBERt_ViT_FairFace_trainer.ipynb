{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VisualBERt-ViT-trainer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i7JbvU1NC5j",
        "outputId": "d36eabcc-65de-4ab2-c18c-02e13b9cff75"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGwLUaUgSL9n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06c9a0a7-e229-4b0a-bbc2-e4eae8d5a109"
      },
      "source": [
        "! pip install transformers\n",
        "! pip install datasets \n",
        "! pip install --upgrade tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.61.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.10.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.12)\n",
            "Requirement already satisfied: tqdm>=4.42 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.61.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.7.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.61.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMzk0W2H_3rB"
      },
      "source": [
        "!pip uninstall -y torch\n",
        "!pip install torch==1.7.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJ5PiuN_4SD4",
        "outputId": "cb3d0fa2-2705-4c32-acc2-ea28522bfbf0"
      },
      "source": [
        "!pip install pytorch-lightning==1.3.8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-lightning==1.3.8\n",
            "  Downloading pytorch_lightning-1.3.8-py3-none-any.whl (813 kB)\n",
            "\u001b[K     |████████████████████████████████| 813 kB 14.9 MB/s \n",
            "\u001b[?25hCollecting tensorboard!=2.5.0,>=2.2.0\n",
            "  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.6 MB 24.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.8) (4.61.2)\n",
            "Requirement already satisfied: PyYAML<=5.4.1,>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.8) (5.4.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.8) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.8) (1.9.0+cu102)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 45.5 MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.2.0\n",
            "  Downloading torchmetrics-0.4.1-py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 65.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.8) (7.1.2)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.8) (2021.7.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.8) (21.0)\n",
            "Collecting pyDeprecate==0.3.0\n",
            "  Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8) (2.23.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 51.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning==1.3.8) (2.4.7)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (57.2.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (1.34.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (0.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (0.4.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (1.32.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch-lightning==1.3.8) (3.7.4.3)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 52.7 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 68.0 MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8) (21.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (3.5.0)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=9c2256de0e091097fab7f32005fb97360f16e5d0a532178213888321f559ad60\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: multidict, yarl, async-timeout, aiohttp, torchmetrics, tensorboard, pyDeprecate, future, pytorch-lightning\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.5.0 requires tensorboard~=2.5, but you have tensorboard 2.4.1 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 future-0.18.2 multidict-5.1.0 pyDeprecate-0.3.0 pytorch-lightning-1.3.8 tensorboard-2.4.1 torchmetrics-0.4.1 yarl-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy4zMdnsM3dw"
      },
      "source": [
        "!unzip -qq /content/drive/MyDrive/Hateful_Memes/hateful_memes.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WsBkc_dbAcfh",
        "outputId": "3ec41f9d-4ac3-4c65-c85b-cf8b6afe2950"
      },
      "source": [
        "import torch\n",
        "torch.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.9.0+cu102'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN8lH9xXSvs5"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cjLM5dPzzaV"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "dirpath = '/content/model-checkpoint'\n",
        "if os.path.exists(dirpath) and os.path.isdir(dirpath):\n",
        "    shutil.rmtree(dirpath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "k7nWls_fSzMi",
        "outputId": "54884e45-4c40-4a40-f55b-2537d6486b02"
      },
      "source": [
        "df_train = pd.read_json('hateful_memes/train.jsonl', lines=True)\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>img</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>42953</td>\n",
              "      <td>img/42953.png</td>\n",
              "      <td>0</td>\n",
              "      <td>its their character not their color that matters</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>23058</td>\n",
              "      <td>img/23058.png</td>\n",
              "      <td>0</td>\n",
              "      <td>don't be afraid to love again everyone is not ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13894</td>\n",
              "      <td>img/13894.png</td>\n",
              "      <td>0</td>\n",
              "      <td>putting bows on your pet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>37408</td>\n",
              "      <td>img/37408.png</td>\n",
              "      <td>0</td>\n",
              "      <td>i love everything and everybody! except for sq...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>82403</td>\n",
              "      <td>img/82403.png</td>\n",
              "      <td>0</td>\n",
              "      <td>everybody loves chocolate chip cookies, even h...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  ...                                               text\n",
              "0  42953  ...   its their character not their color that matters\n",
              "1  23058  ...  don't be afraid to love again everyone is not ...\n",
              "2  13894  ...                           putting bows on your pet\n",
              "3  37408  ...  i love everything and everybody! except for sq...\n",
              "4  82403  ...  everybody loves chocolate chip cookies, even h...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQ1ODk-5OO5h",
        "outputId": "04cbef6b-8223-40ae-a70e-03d1eccfd398"
      },
      "source": [
        "df_train.label.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    5481\n",
              "1    3019\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "XSaxQs7nNYyK",
        "outputId": "8bbe047a-93aa-4575-a144-bd40b1b0d086"
      },
      "source": [
        "val_seen = pd.read_json('hateful_memes/dev_seen.jsonl', lines=True)\n",
        "val_unseen = pd.read_json('hateful_memes/dev_unseen.jsonl', lines=True)\n",
        "df_val = pd.concat([val_seen, val_unseen],axis=0)\n",
        "df_val.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>img</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8291</td>\n",
              "      <td>img/08291.png</td>\n",
              "      <td>1</td>\n",
              "      <td>white people is this a shooting range</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>46971</td>\n",
              "      <td>img/46971.png</td>\n",
              "      <td>1</td>\n",
              "      <td>bravery at its finest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3745</td>\n",
              "      <td>img/03745.png</td>\n",
              "      <td>1</td>\n",
              "      <td>your order comes to $37.50 and your white priv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>83745</td>\n",
              "      <td>img/83745.png</td>\n",
              "      <td>1</td>\n",
              "      <td>it is time.. to send these parasites back to t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>80243</td>\n",
              "      <td>img/80243.png</td>\n",
              "      <td>1</td>\n",
              "      <td>mississippi wind chime</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  ...                                               text\n",
              "0   8291  ...              white people is this a shooting range\n",
              "1  46971  ...                              bravery at its finest\n",
              "2   3745  ...  your order comes to $37.50 and your white priv...\n",
              "3  83745  ...  it is time.. to send these parasites back to t...\n",
              "4  80243  ...                             mississippi wind chime\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tTMcN4tNXAq",
        "outputId": "2be58c07-5961-4b01-94a1-2048641e87a5"
      },
      "source": [
        "test_seen = pd.read_json('hateful_memes/test_seen.jsonl', lines=True)\n",
        "test_unseen = pd.read_json('hateful_memes/test_unseen.jsonl', lines=True)\n",
        "df_test = pd.concat([test_seen, test_unseen],axis=0)\n",
        "df_train.shape, df_val.shape, df_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8500, 4), (1040, 4), (3000, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNZioUGONoFk",
        "outputId": "a361d30a-ce19-4d5c-9af1-6659e2123e50"
      },
      "source": [
        "df_val.label.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    593\n",
              "1    447\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSDdEPzXN8rz",
        "outputId": "1b5cbb88-e2df-45c6-a165-a5cba4c3c75a"
      },
      "source": [
        "df_test.label.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1760\n",
              "1    1240\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "DCqW-BBgO1w5",
        "outputId": "b442180d-0d81-485c-aa3f-24e7afbce22b"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>img</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>42953</td>\n",
              "      <td>img/42953.png</td>\n",
              "      <td>0</td>\n",
              "      <td>its their character not their color that matters</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>23058</td>\n",
              "      <td>img/23058.png</td>\n",
              "      <td>0</td>\n",
              "      <td>don't be afraid to love again everyone is not ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13894</td>\n",
              "      <td>img/13894.png</td>\n",
              "      <td>0</td>\n",
              "      <td>putting bows on your pet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>37408</td>\n",
              "      <td>img/37408.png</td>\n",
              "      <td>0</td>\n",
              "      <td>i love everything and everybody! except for sq...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>82403</td>\n",
              "      <td>img/82403.png</td>\n",
              "      <td>0</td>\n",
              "      <td>everybody loves chocolate chip cookies, even h...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  ...                                               text\n",
              "0  42953  ...   its their character not their color that matters\n",
              "1  23058  ...  don't be afraid to love again everyone is not ...\n",
              "2  13894  ...                           putting bows on your pet\n",
              "3  37408  ...  i love everything and everybody! except for sq...\n",
              "4  82403  ...  everybody loves chocolate chip cookies, even h...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_192_dTP5Sn",
        "outputId": "e6eaf357-46b4-44c9-ac59-934bbac331eb"
      },
      "source": [
        "df_train['text_len'] = df_train['text'].str.split().str.len()\n",
        "df_train['text_len'].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    8500.000000\n",
              "mean       11.742588\n",
              "std         6.877021\n",
              "min         1.000000\n",
              "25%         7.000000\n",
              "50%        10.000000\n",
              "75%        15.000000\n",
              "max        70.000000\n",
              "Name: text_len, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "xmFf_MUKkAFD",
        "outputId": "91827ba1-aca5-4f0b-b8d0-691ec626bf77"
      },
      "source": [
        "df_train['idx'] = df_train['id'].astype(str).str.zfill(5)\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>img</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>text_len</th>\n",
              "      <th>idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>42953</td>\n",
              "      <td>img/42953.png</td>\n",
              "      <td>0</td>\n",
              "      <td>its their character not their color that matters</td>\n",
              "      <td>8</td>\n",
              "      <td>42953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>23058</td>\n",
              "      <td>img/23058.png</td>\n",
              "      <td>0</td>\n",
              "      <td>don't be afraid to love again everyone is not ...</td>\n",
              "      <td>12</td>\n",
              "      <td>23058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13894</td>\n",
              "      <td>img/13894.png</td>\n",
              "      <td>0</td>\n",
              "      <td>putting bows on your pet</td>\n",
              "      <td>5</td>\n",
              "      <td>13894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>37408</td>\n",
              "      <td>img/37408.png</td>\n",
              "      <td>0</td>\n",
              "      <td>i love everything and everybody! except for sq...</td>\n",
              "      <td>11</td>\n",
              "      <td>37408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>82403</td>\n",
              "      <td>img/82403.png</td>\n",
              "      <td>0</td>\n",
              "      <td>everybody loves chocolate chip cookies, even h...</td>\n",
              "      <td>7</td>\n",
              "      <td>82403</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id            img  ...  text_len    idx\n",
              "0  42953  img/42953.png  ...         8  42953\n",
              "1  23058  img/23058.png  ...        12  23058\n",
              "2  13894  img/13894.png  ...         5  13894\n",
              "3  37408  img/37408.png  ...        11  37408\n",
              "4  82403  img/82403.png  ...         7  82403\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7aHKHGRkNG2"
      },
      "source": [
        "df_val['idx'] = df_val['id'].astype(str).str.zfill(5)\n",
        "df_test['idx'] = df_test['id'].astype(str).str.zfill(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_M8zN1groWmC",
        "outputId": "89f4a601-190d-4f91-965b-3e0a9fdeb154"
      },
      "source": [
        "df_train.shape, df_val.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8500, 6), (1040, 5))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_ieiNwrLUzt"
      },
      "source": [
        "## Compute Class Weight"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjGXoyRtLXuY",
        "outputId": "4254f569-b361-441d-bcfa-3ab84e032453"
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "y_train = df_train[\"label\"].values.tolist()\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                 np.unique(y_train),\n",
        "                                                 y_train)\n",
        "print(class_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.77540595 1.40775091]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgUHGzZFP4fG",
        "outputId": "b3926a3f-f884-4902-dd35-d3d59a68122c"
      },
      "source": [
        "df_train.label.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    5481\n",
              "1    3019\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrmDPEtUe7Hm"
      },
      "source": [
        "## Load as a dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvtF2j5CaPn4"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15HVpWiKPSkH",
        "outputId": "9df6f350-3ddc-4a15-b37c-10848612b7e4"
      },
      "source": [
        "from datasets import list_metrics, load_metric\n",
        "metrics_list = list_metrics()\n",
        "print(metrics_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['accuracy', 'bertscore', 'bleu', 'bleurt', 'cer', 'comet', 'coval', 'cuad', 'f1', 'gleu', 'glue', 'indic_glue', 'matthews_correlation', 'meteor', 'pearsonr', 'precision', 'recall', 'rouge', 'sacrebleu', 'sari', 'seqeval', 'spearmanr', 'squad', 'squad_v2', 'super_glue', 'wer', 'wiki_split', 'xnli']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kv4m5UAf1B3"
      },
      "source": [
        "acc_metric = load_metric('accuracy')\n",
        "f1_metric = load_metric('f1')\n",
        "precision_metric = load_metric('precision')\n",
        "recall_metric = load_metric('recall')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZD53AeagF4D"
      },
      "source": [
        "## Create Dataset function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olZeIJOhgDty"
      },
      "source": [
        "from transformers import BertTokenizer, VisualBertForPreTraining, AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ0oB82ISJHA"
      },
      "source": [
        "## Load Visual Embedding features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeeChXlYk1QF"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTD_ZGsbP1qT"
      },
      "source": [
        "from transformers import ViTFeatureExtractor, ViTModel\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlJiS4aLQD1_"
      },
      "source": [
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "feature_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k').to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmgaSV7OdVmd"
      },
      "source": [
        "## Adding in FairFace Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "2F218TnydZ2T",
        "outputId": "b164f0d5-f6ea-466d-f497-6988ef446715"
      },
      "source": [
        "fairface = pd.read_csv('/content/drive/MyDrive/Hateful_Memes/fairface_outputs.csv')\n",
        "fairface.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>face_name_align</th>\n",
              "      <th>race</th>\n",
              "      <th>race4</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>race_scores_fair</th>\n",
              "      <th>race_scores_fair_4</th>\n",
              "      <th>gender_scores_fair</th>\n",
              "      <th>age_scores_fair</th>\n",
              "      <th>bbox</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hateful_memes/img/97453.png</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hateful_memes/img/78239.png</td>\n",
              "      <td>White</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>20-29</td>\n",
              "      <td>[0.46716055 0.03184895 0.27454463 0.08496673 0...</td>\n",
              "      <td>[0.53907245 0.39643455 0.05888477 0.00560816]</td>\n",
              "      <td>[0.05742028 0.9425797 ]</td>\n",
              "      <td>[0.00250324 0.12263069 0.14131351 0.5183566  0...</td>\n",
              "      <td>[(81, 104) (203, 227)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hateful_memes/img/38679.png</td>\n",
              "      <td>East Asian</td>\n",
              "      <td>Asian</td>\n",
              "      <td>Female</td>\n",
              "      <td>10-19</td>\n",
              "      <td>[1.5623933e-02 2.1323907e-04 4.3708492e-02 7.0...</td>\n",
              "      <td>[0.00633232 0.00164239 0.9886304  0.00339485]</td>\n",
              "      <td>[0.11734743 0.8826525 ]</td>\n",
              "      <td>[4.7104387e-04 1.0171387e-01 4.9716425e-01 3.6...</td>\n",
              "      <td>[(163, 86) (320, 242)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hateful_memes/img/82714.png</td>\n",
              "      <td>East Asian</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>20-29</td>\n",
              "      <td>[0.12071104 0.07237878 0.12501504 0.26343024 0...</td>\n",
              "      <td>[0.01736401 0.566375   0.2025124  0.21374854]</td>\n",
              "      <td>[0.6695595  0.33044055]</td>\n",
              "      <td>[0.00421524 0.03200694 0.08180724 0.64399767 0...</td>\n",
              "      <td>[(136, 143) (253, 260)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hateful_memes/img/82647.png</td>\n",
              "      <td>Indian</td>\n",
              "      <td>Asian</td>\n",
              "      <td>Male</td>\n",
              "      <td>20-29</td>\n",
              "      <td>[0.07455086 0.04270947 0.22053772 0.06786881 0...</td>\n",
              "      <td>[0.04457024 0.15466341 0.69744956 0.10331677]</td>\n",
              "      <td>[0.9239432  0.07605679]</td>\n",
              "      <td>[2.1227893e-04 3.8796343e-02 1.2877938e-01 5.3...</td>\n",
              "      <td>[(342, 194) (484, 335)]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               face_name_align  ...                     bbox\n",
              "0  hateful_memes/img/97453.png  ...                      NaN\n",
              "1  hateful_memes/img/78239.png  ...   [(81, 104) (203, 227)]\n",
              "2  hateful_memes/img/38679.png  ...   [(163, 86) (320, 242)]\n",
              "3  hateful_memes/img/82714.png  ...  [(136, 143) (253, 260)]\n",
              "4  hateful_memes/img/82647.png  ...  [(342, 194) (484, 335)]\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkCQ2q7Vd2xb",
        "outputId": "4263f73d-5d4b-4533-80dc-4615c85f75cf"
      },
      "source": [
        "fairface.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12140, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "3ixZdSiReAKq",
        "outputId": "aae1fd56-edf6-49ef-d6b4-e30c9aea451a"
      },
      "source": [
        "fairface['img'] = fairface['face_name_align'].str.replace('hateful_memes/', '')\n",
        "keep_cols= ['img', 'race4', 'gender']\n",
        "fairface = fairface[keep_cols]\n",
        "fairface.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img</th>\n",
              "      <th>race4</th>\n",
              "      <th>gender</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>img/97453.png</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>img/78239.png</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>img/38679.png</td>\n",
              "      <td>Asian</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>img/82714.png</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>img/82647.png</td>\n",
              "      <td>Asian</td>\n",
              "      <td>Male</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             img  race4  gender\n",
              "0  img/97453.png    NaN     NaN\n",
              "1  img/78239.png  White  Female\n",
              "2  img/38679.png  Asian  Female\n",
              "3  img/82714.png  Black    Male\n",
              "4  img/82647.png  Asian    Male"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "v5ARuar2eZTr",
        "outputId": "088ffa05-3583-42d1-b41c-542d6144f108"
      },
      "source": [
        "df_train = pd.merge(df_train, fairface, on=['img'], how='left')\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>img</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>text_len</th>\n",
              "      <th>idx</th>\n",
              "      <th>race4</th>\n",
              "      <th>gender</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>42953</td>\n",
              "      <td>img/42953.png</td>\n",
              "      <td>0</td>\n",
              "      <td>its their character not their color that matters</td>\n",
              "      <td>8</td>\n",
              "      <td>42953</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>23058</td>\n",
              "      <td>img/23058.png</td>\n",
              "      <td>0</td>\n",
              "      <td>don't be afraid to love again everyone is not ...</td>\n",
              "      <td>12</td>\n",
              "      <td>23058</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13894</td>\n",
              "      <td>img/13894.png</td>\n",
              "      <td>0</td>\n",
              "      <td>putting bows on your pet</td>\n",
              "      <td>5</td>\n",
              "      <td>13894</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>37408</td>\n",
              "      <td>img/37408.png</td>\n",
              "      <td>0</td>\n",
              "      <td>i love everything and everybody! except for sq...</td>\n",
              "      <td>11</td>\n",
              "      <td>37408</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>82403</td>\n",
              "      <td>img/82403.png</td>\n",
              "      <td>0</td>\n",
              "      <td>everybody loves chocolate chip cookies, even h...</td>\n",
              "      <td>7</td>\n",
              "      <td>82403</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id            img  label  ...    idx  race4 gender\n",
              "0  42953  img/42953.png      0  ...  42953  Black   Male\n",
              "1  23058  img/23058.png      0  ...  23058  Black   Male\n",
              "2  13894  img/13894.png      0  ...  13894    NaN    NaN\n",
              "3  37408  img/37408.png      0  ...  37408    NaN    NaN\n",
              "4  82403  img/82403.png      0  ...  82403    NaN    NaN\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "IzpuNYCbemlB",
        "outputId": "e9769df4-6053-4a94-e975-6eb8c1b052f0"
      },
      "source": [
        "df_val= pd.merge(df_val, fairface, on=['img'], how='left')\n",
        "df_val.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>img</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>idx</th>\n",
              "      <th>race4</th>\n",
              "      <th>gender</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8291</td>\n",
              "      <td>img/08291.png</td>\n",
              "      <td>1</td>\n",
              "      <td>white people is this a shooting range</td>\n",
              "      <td>08291</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>46971</td>\n",
              "      <td>img/46971.png</td>\n",
              "      <td>1</td>\n",
              "      <td>bravery at its finest</td>\n",
              "      <td>46971</td>\n",
              "      <td>Black</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3745</td>\n",
              "      <td>img/03745.png</td>\n",
              "      <td>1</td>\n",
              "      <td>your order comes to $37.50 and your white priv...</td>\n",
              "      <td>03745</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>83745</td>\n",
              "      <td>img/83745.png</td>\n",
              "      <td>1</td>\n",
              "      <td>it is time.. to send these parasites back to t...</td>\n",
              "      <td>83745</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>80243</td>\n",
              "      <td>img/80243.png</td>\n",
              "      <td>1</td>\n",
              "      <td>mississippi wind chime</td>\n",
              "      <td>80243</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id            img  label  ...    idx  race4  gender\n",
              "0   8291  img/08291.png      1  ...  08291  White    Male\n",
              "1  46971  img/46971.png      1  ...  46971  Black  Female\n",
              "2   3745  img/03745.png      1  ...  03745  White  Female\n",
              "3  83745  img/83745.png      1  ...  83745    NaN     NaN\n",
              "4  80243  img/80243.png      1  ...  80243    NaN     NaN\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6aEEaASf6wg",
        "outputId": "147e0cf3-5466-4292-87dc-b2f042e317d2"
      },
      "source": [
        "df_train['race4'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Black     2093\n",
              "Asian     1725\n",
              "White     1592\n",
              "Indian     367\n",
              "Name: race4, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zE4ZYjYUgHUW",
        "outputId": "b31679e6-9f84-40b2-db86-9de9919cda56"
      },
      "source": [
        "race_list = df_train['race4'].values.tolist()\n",
        "race_list[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Black', 'Black', nan, nan, nan, nan, nan, nan, nan, 'Indian']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pIK2KYvSnh-"
      },
      "source": [
        "class HatefulMemesData(Dataset):\n",
        "    def __init__(self, df, tokenizer, sequence_length, \n",
        "                 print_text=False):         \n",
        "\n",
        "        self.sequence_length = sequence_length\n",
        "        self.tokenizer = tokenizer\n",
        "        self.print_text = print_text\n",
        "\n",
        "        texts = df[\"text\"].values.tolist()\n",
        "        labels = df[\"label\"].values.tolist()\n",
        "        images = df[\"img\"].values.tolist()\n",
        "        ids =  df[\"idx\"].values.tolist()\n",
        "        races = df[\"race4\"].values.tolist()\n",
        "        genders = df[\"gender\"].values.tolist()\n",
        "\n",
        "        self.dataset = []\n",
        "        for i, inp in enumerate(texts):\n",
        "            self.dataset.append({\"text\": inp, \"label\": labels[i], 'idx': ids[i], 'image': images[i],\n",
        "                                 'race': races[i], 'gender': genders[i]})\n",
        "  \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "\n",
        "    def tokenize_data(self, example):\n",
        "   \n",
        "        idx = example['idx']\n",
        "        race = example['race'] \n",
        "        if race == 'nan':\n",
        "            race = ''\n",
        "        gender = example['gender']\n",
        "        if gender == 'nan':\n",
        "            gender = ''\n",
        "        idx = [idx] if isinstance(idx, str) else idx\n",
        "        text_ = example['text'] + ' <s> race: ' + str(race) + '</s>.<s> gender: ' + str(gender) + '</s>'\n",
        "        # encoded_dict = tokenizer.batch_encode_plus(example['text'], padding='max_length', max_length=max_len, truncation=True, return_tensors='pt')\n",
        "        encoded_dict = tokenizer(text_, padding='max_length', max_length=self.sequence_length, truncation=True, return_tensors='pt')\n",
        "        tokens = encoded_dict['input_ids']\n",
        "        token_type_ids = encoded_dict['token_type_ids']\n",
        "        attn_mask = encoded_dict['attention_mask']\n",
        "        \n",
        "        targets = torch.tensor(example['label']).type(torch.int64)\n",
        "\n",
        "        ## Get Visual Embeddings\n",
        "        try:\n",
        "            img = example['image']\n",
        "            img = Image.open(os.path.join('hateful_memes', img))\n",
        "            img = np.array(img)\n",
        "            img = img[...,:3]\n",
        "            inputs = feature_extractor(images=img, return_tensors=\"pt\")\n",
        "            outputs = feature_model(**inputs.to('cuda'))\n",
        "            visual_embeds = outputs.last_hidden_state\n",
        "            visual_embeds = visual_embeds.cpu()\n",
        "        except:\n",
        "            # print(\"Error with Id: \", idx)\n",
        "            visual_embeds = np.zeros(shape=(197, 768), dtype=float)\n",
        "        # visual_embeds = visual_embeds.repeat(1,1,2)\n",
        "\n",
        "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
        "        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
        "\n",
        "        inputs={\"input_ids\": tokens.squeeze(),\n",
        "            \"attention_mask\": attn_mask.squeeze(),\n",
        "            \"token_type_ids\": token_type_ids.squeeze(),\n",
        "            \"visual_embeds\": visual_embeds.squeeze(),\n",
        "            \"visual_token_type_ids\": visual_token_type_ids.squeeze(),\n",
        "            \"visual_attention_mask\": visual_attention_mask.squeeze(),\n",
        "            \"label\": targets.squeeze()\n",
        "        }\n",
        "        \n",
        "        return inputs\n",
        "  \n",
        "    def __getitem__(self, index):\n",
        "        inputs = self.tokenize_data(self.dataset[index])\n",
        "        \n",
        "        if self.print_text:\n",
        "            for k in inputs.keys():\n",
        "                print(k, inputs[k].shape, inputs[k].dtype)\n",
        "\n",
        "        return inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5W2mYO_kLLf"
      },
      "source": [
        "dataset = HatefulMemesData(df_val, tokenizer, 70, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lz3WbW5kUog",
        "outputId": "ea3d3384-e80a-4cc0-b839-3eb032b3b738"
      },
      "source": [
        "example1 = dataset[100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_ids torch.Size([70]) torch.int64\n",
            "attention_mask torch.Size([70]) torch.int64\n",
            "token_type_ids torch.Size([70]) torch.int64\n",
            "visual_embeds torch.Size([197, 768]) torch.float32\n",
            "visual_token_type_ids torch.Size([197]) torch.int64\n",
            "visual_attention_mask torch.Size([197]) torch.int64\n",
            "label torch.Size([]) torch.int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LTa5sqUkI_J"
      },
      "source": [
        "## Fine-Tune Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdLo_MtAkCZt",
        "outputId": "23e40959-389c-424f-f101-e50a0111a812"
      },
      "source": [
        "from transformers import BertTokenizer, VisualBertModel, TrainingArguments, Trainer, VisualBertConfig\n",
        "configuration = VisualBertConfig.from_pretrained('uclanlp/visualbert-vqa-coco-pre',\n",
        "                                                hidden_dropout_prob=0.2, attention_probs_dropout_prob=0.2)\n",
        "model = VisualBertModel.from_pretrained('uclanlp/visualbert-vqa-coco-pre', config=configuration)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at uclanlp/visualbert-vqa-coco-pre were not used when initializing VisualBertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing VisualBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing VisualBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWcMWWgzF71h",
        "outputId": "48f3ea24-01a9-4d26-d54a-54d0a0987cb0"
      },
      "source": [
        "# example1 = tokenize_data(df_train.to_dict('records')[0])\n",
        "print(example1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([  101,  2651,  2057,  2024,  2437, 15415, 11350,  1026,  1055,  1028,\n",
            "         2679,  1024,  2317,  1026,  1013,  1055,  1028,  1012,  1026,  1055,\n",
            "         1028,  5907,  1024,  3287,  1026,  1013,  1055,  1028,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'visual_embeds': tensor([[ 0.0642, -0.0310, -0.0999,  ..., -0.1901, -0.0072, -0.1251],\n",
            "        [ 0.0562,  0.0653,  0.1875,  ..., -0.1634,  0.1641,  0.1187],\n",
            "        [ 0.1889,  0.1166,  0.2313,  ..., -0.0837,  0.2443, -0.0418],\n",
            "        ...,\n",
            "        [ 0.1838,  0.0838, -0.2403,  ...,  0.0042, -0.1254, -0.0066],\n",
            "        [ 0.1517, -0.0088, -0.1847,  ..., -0.1020,  0.0600,  0.0403],\n",
            "        [-0.0082, -0.0115, -0.0076,  ..., -0.0290, -0.0129, -0.0478]],\n",
            "       grad_fn=<SqueezeBackward0>), 'visual_token_type_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1]), 'visual_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1]), 'label': tensor(1)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ou29gT0lgbv2",
        "outputId": "0a6833b0-9217-46d9-d8a1-bab37a7d2844"
      },
      "source": [
        "example1['input_ids'].unsqueeze(0).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 70])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXGtuj6f2XgR"
      },
      "source": [
        "model = model.double()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_1lUf09D91Z"
      },
      "source": [
        "outputs = model(input_ids=example1['input_ids'].unsqueeze(0),\n",
        "                attention_mask=example1['attention_mask'].unsqueeze(0),\n",
        "                visual_token_type_ids=example1['visual_token_type_ids'].unsqueeze(0),\n",
        "                token_type_ids=example1['token_type_ids'].unsqueeze(0),\n",
        "                visual_embeds=example1['visual_embeds'].unsqueeze(0),\n",
        "                visual_attention_mask=example1['visual_attention_mask'].unsqueeze(0),\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67hOqJMSK7GX"
      },
      "source": [
        "pooled_outputs = outputs[1]\n",
        "print(pooled_outputs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAhhNY4y4MUa"
      },
      "source": [
        "## Tuning using Pytorch Lightning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8Nh2K8j4L1g"
      },
      "source": [
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from datasets import load_metric\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    VisualBertModel,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "import logging\n",
        "import argparse\n",
        "import time\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpsDvM-r4L4T"
      },
      "source": [
        "# from pytorch_lightning.loggers.wandb import WandbLogger\n",
        "import os\n",
        "from pathlib import Path\n",
        "from string import punctuation\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-hl24bz3obU"
      },
      "source": [
        "## Look at Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-_UouKgObfk",
        "outputId": "a8295ffe-a4ad-4fca-f79b-c0d766c5a36d"
      },
      "source": [
        "weights = [0.77510622, 1.40873991]\n",
        "wt_tensor = torch.FloatTensor(weights).cuda()\n",
        "print(wt_tensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.7751, 1.4087], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTSuZE5G3FNu"
      },
      "source": [
        "class VisualBERTClassifier(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
        "        member variables.\n",
        "        \"\"\"\n",
        "        super(VisualBERTClassifier, self).__init__()\n",
        "        configuration = VisualBertConfig.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre',\n",
        "                                                hidden_dropout_prob=0.2, attention_probs_dropout_prob=0.2)\n",
        "        self.visualbert = VisualBertModel.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre', config=configuration)\n",
        "        self.embed_cls = nn.Linear(768, 1024)\n",
        "        # self.visualbert = VisualBertModel.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre')\n",
        "        self.num_labels = 2\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.cls=  nn.Linear(768, self.num_labels)\n",
        "        self.weight = torch.FloatTensor([0.77510622, 1.40873991]),\n",
        "\n",
        "        nSamples = [5178, 2849]\n",
        "        normedWeights = [1 - (x / sum(nSamples)) for x in nSamples]\n",
        "        self.loss_fct = CrossEntropyLoss(weight=torch.FloatTensor(normedWeights))\n",
        "        \n",
        "    \n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, visual_embeds, visual_attention_mask,\n",
        "                visual_token_type_ids, labels):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Tensor of input data and we must return\n",
        "        a Tensor of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Tensors.\n",
        "        \"\"\"\n",
        "        visual_embeds_cls = self.embed_cls(visual_embeds)\n",
        "        outputs = self.visualbert(\n",
        "                input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids,\n",
        "                visual_embeds=visual_embeds_cls,\n",
        "                visual_attention_mask=visual_attention_mask,\n",
        "                visual_token_type_ids=visual_token_type_ids,\n",
        "            )\n",
        "        \n",
        "        pooled_output = outputs[1]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.cls(pooled_output)\n",
        "        reshaped_logits = logits.view(-1, self.num_labels)\n",
        "\n",
        "        loss = self.loss_fct(reshaped_logits, labels.view(-1))\n",
        "      \n",
        "        return loss, reshaped_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6v7whCY5Qwy",
        "outputId": "54f86110-9938-4c55-f472-6fde557df4ff"
      },
      "source": [
        "model = VisualBERTClassifier().to('cuda')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at uclanlp/visualbert-nlvr2-coco-pre were not used when initializing VisualBertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing VisualBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing VisualBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnw7jNeFyIEc",
        "outputId": "b83cfe82-1484-429e-95f4-bb509a9dd891"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jul 27 23:58:31 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    35W / 250W |   1909MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7tblaBP7Gb_"
      },
      "source": [
        "## Using HuggingFace Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q64H3ECW7Jq_"
      },
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "batch_size = 36\n",
        "seq_len = 70"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRuUBtBe8JYR",
        "outputId": "78e44e8f-b4db-4091-f966-e62b593c1652"
      },
      "source": [
        "model = VisualBERTClassifier()\n",
        "model = model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at uclanlp/visualbert-nlvr2-coco-pre were not used when initializing VisualBertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing VisualBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing VisualBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UefymyHr7Jt_"
      },
      "source": [
        "metric_name = \"auroc\"\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir = \"model-checkpoint\",\n",
        "    seed = 110, \n",
        "    evaluation_strategy = \"steps\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=25,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_name,\n",
        "    eval_steps = 250,\n",
        "    save_steps = 250,\n",
        "    fp16 = False,\n",
        "    gradient_accumulation_steps = 2\n",
        "\n",
        "\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-C8d6fw7Jwh"
      },
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
        "    auc_score = roc_auc_score(labels, predictions)\n",
        "    return {\"accuracy\": acc['accuracy'], \"auroc\": auc_score} "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwSU1fNp70mt"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset = HatefulMemesData(df_train,tokenizer=tokenizer, sequence_length=seq_len),\n",
        "    eval_dataset =  HatefulMemesData(df_val,tokenizer=tokenizer, sequence_length=seq_len),\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so32xliGkBKB"
      },
      "source": [
        "## Best accuracy\n",
        "* NLVR2 : Val Acc: 62.21% , ROC:0.5959"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8c7lrnLe9lUO",
        "outputId": "11461b63-ee01-4660-b092-186fb6ad0330"
      },
      "source": [
        "## To resume from an old checkpoint, set path in resume-from\n",
        "resume_from ='/content/model-checkpoint/checkpoint-12000'\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 8500\n",
            "  Num Epochs = 25\n",
            "  Instantaneous batch size per device = 36\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 72\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 2950\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2950' max='2950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2950/2950 6:09:57, Epoch 24/25]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Auroc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.713055</td>\n",
              "      <td>0.564423</td>\n",
              "      <td>0.553876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.659700</td>\n",
              "      <td>0.749445</td>\n",
              "      <td>0.581731</td>\n",
              "      <td>0.580620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.659700</td>\n",
              "      <td>1.063063</td>\n",
              "      <td>0.587500</td>\n",
              "      <td>0.575214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.359600</td>\n",
              "      <td>1.291010</td>\n",
              "      <td>0.608654</td>\n",
              "      <td>0.600098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.359600</td>\n",
              "      <td>1.644730</td>\n",
              "      <td>0.620192</td>\n",
              "      <td>0.594793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.132700</td>\n",
              "      <td>1.897707</td>\n",
              "      <td>0.608654</td>\n",
              "      <td>0.586328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.132700</td>\n",
              "      <td>2.131607</td>\n",
              "      <td>0.604808</td>\n",
              "      <td>0.583231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.059900</td>\n",
              "      <td>2.431339</td>\n",
              "      <td>0.618269</td>\n",
              "      <td>0.589802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.059900</td>\n",
              "      <td>2.668790</td>\n",
              "      <td>0.614423</td>\n",
              "      <td>0.588082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.023000</td>\n",
              "      <td>3.156004</td>\n",
              "      <td>0.610577</td>\n",
              "      <td>0.574244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>0.023000</td>\n",
              "      <td>3.064539</td>\n",
              "      <td>0.618269</td>\n",
              "      <td>0.589802</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1040\n",
            "  Batch size = 36\n",
            "Saving model checkpoint to model-checkpoint/checkpoint-250\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in model-checkpoint/checkpoint-250/tokenizer_config.json\n",
            "Special tokens file saved in model-checkpoint/checkpoint-250/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1040\n",
            "  Batch size = 36\n",
            "Saving model checkpoint to model-checkpoint/checkpoint-500\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in model-checkpoint/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in model-checkpoint/checkpoint-500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1040\n",
            "  Batch size = 36\n",
            "Saving model checkpoint to model-checkpoint/checkpoint-750\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in model-checkpoint/checkpoint-750/tokenizer_config.json\n",
            "Special tokens file saved in model-checkpoint/checkpoint-750/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1040\n",
            "  Batch size = 36\n",
            "Saving model checkpoint to model-checkpoint/checkpoint-1000\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in model-checkpoint/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in model-checkpoint/checkpoint-1000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1040\n",
            "  Batch size = 36\n",
            "Saving model checkpoint to model-checkpoint/checkpoint-1250\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in model-checkpoint/checkpoint-1250/tokenizer_config.json\n",
            "Special tokens file saved in model-checkpoint/checkpoint-1250/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1040\n",
            "  Batch size = 36\n",
            "Saving model checkpoint to model-checkpoint/checkpoint-1500\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in model-checkpoint/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in model-checkpoint/checkpoint-1500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1040\n",
            "  Batch size = 36\n",
            "Saving model checkpoint to model-checkpoint/checkpoint-1750\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in model-checkpoint/checkpoint-1750/tokenizer_config.json\n",
            "Special tokens file saved in model-checkpoint/checkpoint-1750/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1040\n",
            "  Batch size = 36\n",
            "Saving model checkpoint to model-checkpoint/checkpoint-2000\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in model-checkpoint/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in model-checkpoint/checkpoint-2000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1040\n",
            "  Batch size = 36\n",
            "Saving model checkpoint to model-checkpoint/checkpoint-2250\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in model-checkpoint/checkpoint-2250/tokenizer_config.json\n",
            "Special tokens file saved in model-checkpoint/checkpoint-2250/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1040\n",
            "  Batch size = 36\n",
            "Saving model checkpoint to model-checkpoint/checkpoint-2500\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in model-checkpoint/checkpoint-2500/tokenizer_config.json\n",
            "Special tokens file saved in model-checkpoint/checkpoint-2500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1040\n",
            "  Batch size = 36\n",
            "Saving model checkpoint to model-checkpoint/checkpoint-2750\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in model-checkpoint/checkpoint-2750/tokenizer_config.json\n",
            "Special tokens file saved in model-checkpoint/checkpoint-2750/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from model-checkpoint/checkpoint-1000 (score: 0.6000977096702393).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2950, training_loss=0.21083543712809935, metrics={'train_runtime': 22204.1141, 'train_samples_per_second': 9.57, 'train_steps_per_second': 0.133, 'total_flos': 0.0, 'train_loss': 0.21083543712809935, 'epoch': 25.0})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "NIzbDwJeY08R",
        "outputId": "a5655ff5-e1ff-4018-f090-38badc401c3d"
      },
      "source": [
        "import numpy as np\n",
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1040\n",
            "  Batch size = 36\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='29' max='29' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [29/29 01:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 25.0,\n",
              " 'eval_accuracy': 0.6086538461538461,\n",
              " 'eval_auroc': 0.6000977096702393,\n",
              " 'eval_loss': 1.2910103797912598,\n",
              " 'eval_runtime': 67.0334,\n",
              " 'eval_samples_per_second': 15.515,\n",
              " 'eval_steps_per_second': 0.433}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pm3hl5JY3k8",
        "outputId": "a25a732c-1b86-407f-d049-80c8ae9c625e"
      },
      "source": [
        "trainer.save_model('VisualBERT_classification_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to VisualBERT_classification_model\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in VisualBERT_classification_model/tokenizer_config.json\n",
            "Special tokens file saved in VisualBERT_classification_model/special_tokens_map.json\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G58y74pJY7uK",
        "outputId": "a5a66693-b2fb-48d9-bcf9-2043dfdb362b"
      },
      "source": [
        "!zip -r 'VisualBERT_classification_model.zip' 'VisualBERT_classification_model'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: VisualBERT_classification_model/ (stored 0%)\n",
            "  adding: VisualBERT_classification_model/special_tokens_map.json (deflated 40%)\n",
            "  adding: VisualBERT_classification_model/training_args.bin (deflated 47%)\n",
            "  adding: VisualBERT_classification_model/tokenizer.json (deflated 59%)\n",
            "  adding: VisualBERT_classification_model/vocab.txt (deflated 53%)\n",
            "  adding: VisualBERT_classification_model/tokenizer_config.json (deflated 39%)\n",
            "  adding: VisualBERT_classification_model/pytorch_model.bin (deflated 8%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O41GPWcFd3mJ"
      },
      "source": [
        "!mv VisualBERT_classification_model.zip /content/drive/MyDrive/Hateful_Memes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hJiCXVyYn99"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPXnT1KPYo6a"
      },
      "source": [
        "## Pytorch Lightning version of code - May have bugs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Vbp3YOQ4L69"
      },
      "source": [
        "class VisualBERTFineTuner(pl.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super(VisualBERTFineTuner, self).__init__()      \n",
        "        self.model = VisualBERTClassifier().double()\n",
        "        self.num_labels = 2\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(768, self.num_labels)\n",
        "\n",
        "        self.problem_type = 'single_label_classification'\n",
        "        self.save_hyperparameters(hparams)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.output_dir = Path(self.hparams.output_dir)\n",
        "        self.total_steps = 0\n",
        "\n",
        "    def is_logger(self):\n",
        "        return self.trainer.global_rank <= 0\n",
        "    \n",
        "        \n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, visual_embeds, visual_attention_mask,\n",
        "                visual_token_type_ids, labels):\n",
        "        loss, preds = self.model(\n",
        "                input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids,\n",
        "                visual_embeds=visual_embeds,\n",
        "                visual_attention_mask=visual_attention_mask,\n",
        "                visual_token_type_ids=visual_token_type_ids,\n",
        "                labels = labels\n",
        "            )\n",
        "        return loss, preds \n",
        "\n",
        "    # def loss(self, batch, prediction):\n",
        "    #     loss_fct = CrossEntropyLoss()\n",
        "    #     labels = batch['label']\n",
        "    #     loss = loss_fct(prediction.view(-1, self.num_labels), labels.view(-1))\n",
        "    #     return loss\n",
        "   \n",
        "\n",
        "    def _step(self, batch):\n",
        "        outputs = self(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"],\n",
        "            token_type_ids=batch[\"token_type_ids\"],\n",
        "            visual_embeds=batch[\"visual_embeds\"],\n",
        "            visual_attention_mask=batch[\"visual_attention_mask\"],\n",
        "            visual_token_type_ids=batch[\"visual_token_type_ids\"],\n",
        "            labels = batch['label']\n",
        "        )\n",
        "\n",
        "        return outputs\n",
        "    \n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, preds = self._step(batch)\n",
        "        return loss\n",
        "  \n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        val_loss, preds = self._step(batch)\n",
        "        preds = torch.argmax(preds, axis=1)\n",
        "        labels = batch[\"label\"]\n",
        "        return {'loss': val_loss, \"preds\": preds, \"labels\": labels}\n",
        "\n",
        "    \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        preds = torch.cat([x['preds'] for x in outputs]).detach().cpu().numpy()\n",
        "        labels = torch.cat([x['labels'] for x in outputs]).detach().cpu().numpy()\n",
        "        loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
        "        auc_score = roc_auc_score(labels, preds, average='weighted')\n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "        self.log('val_auroc', auc_score, prog_bar=True)\n",
        "        self.log_dict(acc_metric.compute(predictions=preds, references=labels), prog_bar=True)\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "\n",
        "        model = self.model\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": self.hparams.weight_decay,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
        "\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=self.total_steps\n",
        "        )\n",
        "        scheduler = {'scheduler': scheduler, 'interval': 'step', 'frequency': 1}\n",
        "        return [optimizer], [scheduler]\n",
        "  \n",
        "    # def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx,\n",
        "    #                    closure, on_tpu=False, using_native_amp=False, using_lbfgs=False):\n",
        "    #     model = self.model\n",
        "    #     if self.trainer.use_tpu:\n",
        "    #         xm.optimizer_step(optimizer)\n",
        "    #     else:\n",
        "    #         optimizer.step(closure=closure)\n",
        "    #     optimizer.zero_grad()\n",
        "    #     torch.nn.utils.clip_grad_norm_(model.parameters(), self.hparams.max_grad_norm)\n",
        "    #     self.lr_scheduler.step()\n",
        "  \n",
        "    def get_tqdm_dict(self):\n",
        "        tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
        "\n",
        "        return tqdm_dict\n",
        "    \n",
        "    def train_dataloader(self):   \n",
        "        train_loader = DataLoader(HatefulMemesData(df_train, self.tokenizer, self.hparams.max_input_length),\n",
        "                                batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True, num_workers=2)\n",
        "       # Calculate total steps\n",
        "        tb_size = self.hparams.train_batch_size * max(1, self.trainer.gpus)\n",
        "        ab_size = self.trainer.accumulate_grad_batches * float(self.trainer.max_epochs)\n",
        "        self.total_steps = (len(train_loader.dataset) // tb_size) // ab_size\n",
        "\n",
        "        return train_loader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(HatefulMemesData(df_val, self.tokenizer, self.hparams.max_input_length),\n",
        "                          batch_size=self.hparams.eval_batch_size, num_workers=2)\n",
        "    \n",
        "    def test_dataloader(self):\n",
        "        return  DataLoader(HatefulMemesData(df_test, self.tokenizer, self.hparams.max_input_length),\n",
        "                          batch_size=self.hparams.eval_batch_size, num_workers=2)\n",
        "    \n",
        "    def on_save_checkpoint(self, checkpoint):\n",
        "        save_path = self.output_dir.joinpath(model_prefix)\n",
        "        self.model.config.save_step = self.step_count\n",
        "        self.model.save_pretrained(save_path)\n",
        "        self.tokenizer.save_pretrained(save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2H1WD8L-KZw"
      },
      "source": [
        "# from pytorch_lightning import loggers as pl_loggers\n",
        "# tb_logger = pl_loggers.TensorBoardLogger('logs/')\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "class LoggingCallback(pl.Callback):\n",
        "  def on_validation_end(self, trainer, pl_module):\n",
        "    logger.info(\"***** Validation results *****\")\n",
        "    if pl_module.is_logger():\n",
        "      metrics = trainer.callback_metrics\n",
        "      # Log results\n",
        "      for key in sorted(metrics):\n",
        "        if key not in [\"log\", \"progress_bar\"]:\n",
        "          logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "\n",
        "  def on_test_end(self, trainer, pl_module):\n",
        "    logger.info(\"***** Test results *****\")\n",
        "\n",
        "    if pl_module.is_logger():\n",
        "      metrics = trainer.callback_metrics\n",
        "\n",
        "      # Log and save results to file\n",
        "      output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n",
        "      with open(output_test_results_file, \"w\") as writer:\n",
        "        for key in sorted(metrics):\n",
        "          if key not in [\"log\", \"progress_bar\"]:\n",
        "            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnCCYhNV-7Cy"
      },
      "source": [
        "model_name = \"visualbert\"\n",
        "token_len = 50\n",
        "model_prefix = f\"{model_name}-{token_len}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2rdAUs0-Pbj"
      },
      "source": [
        "args_dict = dict(\n",
        "    output_dir=\"\", # path to save the checkpoints\n",
        "    max_input_length=token_len,\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=0.0,\n",
        "    adam_epsilon=1e-8,\n",
        "    warmup_steps=0,\n",
        "    train_batch_size=4,\n",
        "    eval_batch_size=4,\n",
        "    num_train_epochs=5,\n",
        "    gradient_accumulation_steps=1,\n",
        "    n_gpu=1,\n",
        "    resume_from_checkpoint=None, \n",
        "    val_check_interval = 0.5, \n",
        "    early_stop_callback=False,\n",
        "    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
        "    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
        "    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "\n",
        "args_dict.update({'output_dir': \"./\" + model_prefix + \"_final\", 'num_train_epochs':6,\n",
        "             'train_batch_size': 32, 'eval_batch_size': 32})\n",
        "args = argparse.Namespace(**args_dict)\n",
        "\n",
        "\n",
        "## Define Checkpoint function\n",
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "    dirpath=\"./\" + model_prefix + \"_checkpoint\", filename=model_prefix, monitor=\"accuracy\", mode=\"max\", save_top_k=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH5bESY5vVoe",
        "outputId": "dbed693e-e5c3-4f5b-8849-5f79596fa616"
      },
      "source": [
        "print(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(adam_epsilon=1e-08, early_stop_callback=False, eval_batch_size=32, fp_16=False, gradient_accumulation_steps=1, learning_rate=0.0001, max_grad_norm=1.0, max_input_length=50, n_gpu=1, num_train_epochs=6, opt_level='O1', output_dir='./visualbert-50_final', resume_from_checkpoint=None, seed=42, train_batch_size=32, val_check_interval=0.5, warmup_steps=0, weight_decay=0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTjBlV4l-q_N"
      },
      "source": [
        "train_params = dict(\n",
        "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
        "    gpus=min(1, torch.cuda.device_count()),\n",
        "    max_epochs=args.num_train_epochs,\n",
        "    precision= 16 if args.fp_16 else 32,\n",
        "    amp_level=args.opt_level,\n",
        "    resume_from_checkpoint=args.resume_from_checkpoint,\n",
        "    # gradient_clip_val=args.max_grad_norm,\n",
        "    checkpoint_callback=checkpoint_callback,\n",
        "    val_check_interval=args.val_check_interval,\n",
        "    callbacks=[LoggingCallback()],\n",
        "    # logger=tb_logger\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwmhgyIB-r5e"
      },
      "source": [
        "model = VisualBERTFineTuner(args)\n",
        "trainer = pl.Trainer(**train_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1x3bI6Sw6Rb"
      },
      "source": [
        "trainer.fit(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAhPxMZx6kg3"
      },
      "source": [
        "## Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOB-4LmLasvz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29499e63-74d0-437d-8d55-08977280d48a"
      },
      "source": [
        "! pip install optuna -q\n",
        "! pip install 'ray[tune]'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ray[tune] in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (5.4.1)\n",
            "Requirement already satisfied: gpustat in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (0.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.7.4.post0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.0.12)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (0.11.0)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (0.3.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (2.23.0)\n",
            "Requirement already satisfied: aiohttp-cors in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (0.7.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (2.6.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (0.4.4)\n",
            "Requirement already satisfied: pydantic>=1.8 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.8.2)\n",
            "Requirement already satisfied: redis>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.5.3)\n",
            "Requirement already satisfied: opencensus in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (0.7.13)\n",
            "Requirement already satisfied: aioredis in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.3.1)\n",
            "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.34.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (0.8.9)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (2.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.1.5)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.28.1->ray[tune]) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic>=1.8->ray[tune]) (3.7.4.3)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray[tune]) (3.0.1)\n",
            "Requirement already satisfied: chardet<5.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray[tune]) (3.0.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray[tune]) (5.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray[tune]) (1.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray[tune]) (21.2.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->ray[tune]) (2.10)\n",
            "Requirement already satisfied: hiredis in /usr/local/lib/python3.7/dist-packages (from aioredis->ray[tune]) (2.0.0)\n",
            "Requirement already satisfied: blessings>=1.6 in /usr/local/lib/python3.7/dist-packages (from gpustat->ray[tune]) (1.7)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from gpustat->ray[tune]) (7.352.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from gpustat->ray[tune]) (5.4.8)\n",
            "Requirement already satisfied: opencensus-context==0.1.2 in /usr/local/lib/python3.7/dist-packages (from opencensus->ray[tune]) (0.1.2)\n",
            "Requirement already satisfied: google-api-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from opencensus->ray[tune]) (1.26.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (1.53.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (2018.9)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (21.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (57.2.0)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (1.32.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (4.2.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (2021.5.30)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[tune]) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehKa09yWRBJE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "d56fa08d-4aea-46ec-9284-7fc27682a223"
      },
      "source": [
        "import ray\n",
        "ray.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "  \"update your install command.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.5.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a8IrKkuZRy7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bbc2494-475c-4aff-cb43-ba14919c9b0d"
      },
      "source": [
        "! pip install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.10.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.7.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.12)\n",
            "Requirement already satisfied: tqdm>=4.42 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.61.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6RyfM1aaHaS"
      },
      "source": [
        "def model_init():\n",
        "    return VisualBERTClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahzqsEkJaXEz",
        "outputId": "9d9a6453-f429-4de0-dc8a-c425101644f2"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=args,\n",
        "    train_dataset = HatefulMemesData(df_train,tokenizer=tokenizer, sequence_length=seq_len),\n",
        "    eval_dataset =  HatefulMemesData(df_val,tokenizer=tokenizer, sequence_length=seq_len),\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/uclanlp/visualbert-nlvr2-coco-pre/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/122e8cd00b301567659d3353a2ed513c61c15fccd00379a2119fb8e26ca36ba8.0400f2bf7185eb82b2e7b236bf78461c11b103f742f4df05c36fce4fc8e08e19\n",
            "Model config VisualBertConfig {\n",
            "  \"architectures\": [\n",
            "    \"VisualBertForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.2,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"bypass_transformer\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.2,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"visual_bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"special_visual_initialize\": true,\n",
            "  \"transformers_version\": \"4.9.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"visual_embedding_dim\": 1024,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/uclanlp/visualbert-nlvr2-coco-pre/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/05411b1f5b39a98797e5a9019114736fec4006c64ef2d3076915872149e2828c.b105c7b4cdd5aa4b67e166e6326aaad61d996b5314cdc9546d665ae72c1ff8ce\n",
            "Some weights of the model checkpoint at uclanlp/visualbert-nlvr2-coco-pre were not used when initializing VisualBertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing VisualBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing VisualBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of VisualBertModel were initialized from the model checkpoint at uclanlp/visualbert-nlvr2-coco-pre.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use VisualBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5GBNZHVzawUO",
        "outputId": "74ff538f-1136-4697-f068-ec4d93e866ae"
      },
      "source": [
        "best_run = trainer.hyperparameter_search(n_trials=10, direction=\"maximize\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-07-28 00:02:04,878]\u001b[0m A new study created in memory with name: no-name-0961d1bd-def0-44a5-aab4-801e44e6372e\u001b[0m\n",
            "Trial:\n",
            "loading configuration file https://huggingface.co/uclanlp/visualbert-nlvr2-coco-pre/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/122e8cd00b301567659d3353a2ed513c61c15fccd00379a2119fb8e26ca36ba8.0400f2bf7185eb82b2e7b236bf78461c11b103f742f4df05c36fce4fc8e08e19\n",
            "Model config VisualBertConfig {\n",
            "  \"architectures\": [\n",
            "    \"VisualBertForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.2,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"bypass_transformer\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.2,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"visual_bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"special_visual_initialize\": true,\n",
            "  \"transformers_version\": \"4.9.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"visual_embedding_dim\": 1024,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/uclanlp/visualbert-nlvr2-coco-pre/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/05411b1f5b39a98797e5a9019114736fec4006c64ef2d3076915872149e2828c.b105c7b4cdd5aa4b67e166e6326aaad61d996b5314cdc9546d665ae72c1ff8ce\n",
            "Some weights of the model checkpoint at uclanlp/visualbert-nlvr2-coco-pre were not used when initializing VisualBertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing VisualBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing VisualBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of VisualBertModel were initialized from the model checkpoint at uclanlp/visualbert-nlvr2-coco-pre.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use VisualBertModel for predictions without further training.\n",
            "***** Running training *****\n",
            "  Num examples = 8500\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 1593\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1593' max='1593' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1593/1593 42:14, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Auroc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.694239</td>\n",
              "      <td>0.552885</td>\n",
              "      <td>0.528060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.706800</td>\n",
              "      <td>0.693621</td>\n",
              "      <td>0.573077</td>\n",
              "      <td>0.542187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.706800</td>\n",
              "      <td>0.699001</td>\n",
              "      <td>0.585577</td>\n",
              "      <td>0.541581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>0.696157</td>\n",
              "      <td>0.588462</td>\n",
              "      <td>0.561185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>0.704115</td>\n",
              "      <td>0.584615</td>\n",
              "      <td>0.549826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.682900</td>\n",
              "      <td>0.700380</td>\n",
              "      <td>0.580769</td>\n",
              "      <td>0.555266</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1040\n",
            "  Batch size = 36\n",
            "Saving model checkpoint to model-checkpoint/run-0/checkpoint-250\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in model-checkpoint/run-0/checkpoint-250/tokenizer_config.json\n",
            "Special tokens file saved in model-checkpoint/run-0/checkpoint-250/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1040\n",
            "  Batch size = 36\n",
            "Saving model checkpoint to model-checkpoint/run-0/checkpoint-500\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in model-checkpoint/run-0/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in model-checkpoint/run-0/checkpoint-500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1040\n",
            "  Batch size = 36\n",
            "Saving model checkpoint to model-checkpoint/run-0/checkpoint-750\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in model-checkpoint/run-0/checkpoint-750/tokenizer_config.json\n",
            "Special tokens file saved in model-checkpoint/run-0/checkpoint-750/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1040\n",
            "  Batch size = 36\n",
            "Saving model checkpoint to model-checkpoint/run-0/checkpoint-1000\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in model-checkpoint/run-0/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in model-checkpoint/run-0/checkpoint-1000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1040\n",
            "  Batch size = 36\n",
            "Saving model checkpoint to model-checkpoint/run-0/checkpoint-1250\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in model-checkpoint/run-0/checkpoint-1250/tokenizer_config.json\n",
            "Special tokens file saved in model-checkpoint/run-0/checkpoint-1250/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1040\n",
            "  Batch size = 36\n",
            "Saving model checkpoint to model-checkpoint/run-0/checkpoint-1500\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in model-checkpoint/run-0/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in model-checkpoint/run-0/checkpoint-1500/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from model-checkpoint/run-0/checkpoint-1000 (score: 0.5611854936979148).\n",
            "\u001b[32m[I 2021-07-28 00:44:22,406]\u001b[0m Trial 0 finished with value: 1.1360355556406803 and parameters: {'learning_rate': 1.0748855844610069e-06, 'num_train_epochs': 3, 'seed': 25, 'per_device_train_batch_size': 8}. Best is trial 0 with value: 1.1360355556406803.\u001b[0m\n",
            "Trial:\n",
            "loading configuration file https://huggingface.co/uclanlp/visualbert-nlvr2-coco-pre/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/122e8cd00b301567659d3353a2ed513c61c15fccd00379a2119fb8e26ca36ba8.0400f2bf7185eb82b2e7b236bf78461c11b103f742f4df05c36fce4fc8e08e19\n",
            "Model config VisualBertConfig {\n",
            "  \"architectures\": [\n",
            "    \"VisualBertForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.2,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"bypass_transformer\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.2,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"visual_bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"special_visual_initialize\": true,\n",
            "  \"transformers_version\": \"4.9.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"visual_embedding_dim\": 1024,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/uclanlp/visualbert-nlvr2-coco-pre/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/05411b1f5b39a98797e5a9019114736fec4006c64ef2d3076915872149e2828c.b105c7b4cdd5aa4b67e166e6326aaad61d996b5314cdc9546d665ae72c1ff8ce\n",
            "Some weights of the model checkpoint at uclanlp/visualbert-nlvr2-coco-pre were not used when initializing VisualBertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing VisualBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing VisualBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of VisualBertModel were initialized from the model checkpoint at uclanlp/visualbert-nlvr2-coco-pre.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use VisualBertModel for predictions without further training.\n",
            "***** Running training *****\n",
            "  Num examples = 8500\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 1062\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1062' max='1062' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1062/1062 16:49, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Auroc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.703726</td>\n",
              "      <td>0.570192</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.711400</td>\n",
              "      <td>0.694819</td>\n",
              "      <td>0.594231</td>\n",
              "      <td>0.568998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.711400</td>\n",
              "      <td>0.717336</td>\n",
              "      <td>0.591346</td>\n",
              "      <td>0.535349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.696600</td>\n",
              "      <td>0.689823</td>\n",
              "      <td>0.567308</td>\n",
              "      <td>0.556681</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1040\n",
            "  Batch size = 36\n",
            "Saving model checkpoint to model-checkpoint/run-1/checkpoint-250\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in model-checkpoint/run-1/checkpoint-250/tokenizer_config.json\n",
            "Special tokens file saved in model-checkpoint/run-1/checkpoint-250/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1040\n",
            "  Batch size = 36\n",
            "Saving model checkpoint to model-checkpoint/run-1/checkpoint-500\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in model-checkpoint/run-1/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in model-checkpoint/run-1/checkpoint-500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1040\n",
            "  Batch size = 36\n",
            "Saving model checkpoint to model-checkpoint/run-1/checkpoint-750\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in model-checkpoint/run-1/checkpoint-750/tokenizer_config.json\n",
            "Special tokens file saved in model-checkpoint/run-1/checkpoint-750/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1040\n",
            "  Batch size = 36\n",
            "Saving model checkpoint to model-checkpoint/run-1/checkpoint-1000\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in model-checkpoint/run-1/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in model-checkpoint/run-1/checkpoint-1000/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from model-checkpoint/run-1/checkpoint-500 (score: 0.568998494742918).\n",
            "\u001b[32m[I 2021-07-28 01:01:21,413]\u001b[0m Trial 1 finished with value: 1.1239887324818343 and parameters: {'learning_rate': 3.200077625766475e-05, 'num_train_epochs': 1, 'seed': 1, 'per_device_train_batch_size': 4}. Best is trial 0 with value: 1.1360355556406803.\u001b[0m\n",
            "Trial:\n",
            "loading configuration file https://huggingface.co/uclanlp/visualbert-nlvr2-coco-pre/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/122e8cd00b301567659d3353a2ed513c61c15fccd00379a2119fb8e26ca36ba8.0400f2bf7185eb82b2e7b236bf78461c11b103f742f4df05c36fce4fc8e08e19\n",
            "Model config VisualBertConfig {\n",
            "  \"architectures\": [\n",
            "    \"VisualBertForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.2,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"bypass_transformer\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.2,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"visual_bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"special_visual_initialize\": true,\n",
            "  \"transformers_version\": \"4.9.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"visual_embedding_dim\": 1024,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/uclanlp/visualbert-nlvr2-coco-pre/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/05411b1f5b39a98797e5a9019114736fec4006c64ef2d3076915872149e2828c.b105c7b4cdd5aa4b67e166e6326aaad61d996b5314cdc9546d665ae72c1ff8ce\n",
            "Some weights of the model checkpoint at uclanlp/visualbert-nlvr2-coco-pre were not used when initializing VisualBertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing VisualBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing VisualBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of VisualBertModel were initialized from the model checkpoint at uclanlp/visualbert-nlvr2-coco-pre.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use VisualBertModel for predictions without further training.\n",
            "***** Running training *****\n",
            "  Num examples = 8500\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 798\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='595' max='798' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [595/798 28:10 < 09:38, 0.35 it/s, Epoch 2.23/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Auroc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.690967</td>\n",
              "      <td>0.529808</td>\n",
              "      <td>0.529856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.688500</td>\n",
              "      <td>0.707091</td>\n",
              "      <td>0.546154</td>\n",
              "      <td>0.527666</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1040\n",
            "  Batch size = 36\n",
            "Saving model checkpoint to model-checkpoint/run-2/checkpoint-250\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in model-checkpoint/run-2/checkpoint-250/tokenizer_config.json\n",
            "Special tokens file saved in model-checkpoint/run-2/checkpoint-250/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1040\n",
            "  Batch size = 36\n",
            "Saving model checkpoint to model-checkpoint/run-2/checkpoint-500\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in model-checkpoint/run-2/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in model-checkpoint/run-2/checkpoint-500/special_tokens_map.json\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci74nEW2iYLO"
      },
      "source": [
        "import os\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import PopulationBasedTraining\n",
        "from transformers import  AutoConfig, \\\n",
        "    AutoModelForSequenceClassification, AutoTokenizer, Trainer, \\\n",
        "     TrainingArguments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vsUnjoaklxn"
      },
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        " \n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
        "    # precision = precision_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
        "    # recall = recall_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeCfBcgqigqi"
      },
      "source": [
        "def tune_transformer( train_dataset,\n",
        "                     test_dataset,\n",
        "                    num_samples=8,\n",
        "                     gpus_per_trial=0,\n",
        "                     num_labels=5,\n",
        "                     ray_address=None):\n",
        "    \n",
        "    #ray.shutdown()\n",
        "    #ray.init(ray_address, log_to_driver=False)\n",
        "    data_dir_name = \"./data\" \n",
        "    data_dir = os.path.abspath(os.path.join(os.getcwd(), data_dir_name))\n",
        "    if not os.path.exists(data_dir):\n",
        "        os.mkdir(data_dir, 0o755)\n",
        "\n",
        "    # Change these as needed.\n",
        "    model_name = \"roberta-base\" \n",
        "\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        model_name, num_labels=num_labels )\n",
        "\n",
        "    # Download and cache tokenizer, model, and features\n",
        "    print(\"Downloading and caching Tokenizer\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Triggers tokenizer download to cache\n",
        "    print(\"Downloading and caching pre-trained model\")\n",
        "    AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        config=config,\n",
        "    )\n",
        "\n",
        "    def get_model():\n",
        "        return AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            config=config,\n",
        "        )\n",
        "\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\".\",\n",
        "        learning_rate=1e-5,  # config\n",
        "        do_train=True,\n",
        "        do_eval=True,\n",
        "        no_cuda=gpus_per_trial <= 0,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        num_train_epochs=2,  # config\n",
        "        max_steps=-1,\n",
        "        per_device_train_batch_size=16,  # config\n",
        "        per_device_eval_batch_size=16,  # config\n",
        "        warmup_steps=0,\n",
        "        weight_decay=0.1,  # config\n",
        "        logging_dir=\"./logs\",\n",
        "    )\n",
        "\n",
        "    training_args._n_gpu = gpus_per_trial\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model_init=get_model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        compute_metrics=compute_metrics)\n",
        "\n",
        "    tune_config = {\n",
        "        \"per_device_train_batch_size\": 32,\n",
        "        \"per_device_eval_batch_size\": 32,\n",
        "        \"num_train_epochs\": tune.choice([2, 3, 4, 5]),\n",
        "        \"max_steps\": 1 \n",
        "    }\n",
        "\n",
        "    scheduler = PopulationBasedTraining(\n",
        "        time_attr=\"training_iteration\",\n",
        "        metric=\"eval_acc\",\n",
        "        mode=\"max\",\n",
        "        perturbation_interval=1,\n",
        "        hyperparam_mutations={\n",
        "            \"weight_decay\": tune.uniform(0.0, 0.3),\n",
        "            \"learning_rate\": tune.uniform(1e-5, 5e-5),\n",
        "            \"per_device_train_batch_size\": [16, 32, 64],\n",
        "        })\n",
        "\n",
        "    reporter = CLIReporter(\n",
        "        parameter_columns={\n",
        "            \"weight_decay\": \"w_decay\",\n",
        "            \"learning_rate\": \"lr\",\n",
        "            \"per_device_train_batch_size\": \"train_bs/gpu\",\n",
        "            \"num_train_epochs\": \"num_epochs\"\n",
        "        },\n",
        "        metric_columns=[\n",
        "            \"eval_acc\", \"eval_loss\", \"epoch\", \"training_iteration\"\n",
        "        ])\n",
        "\n",
        "    trainer.hyperparameter_search(\n",
        "        hp_space=lambda _: tune_config,\n",
        "        backend=\"ray\",\n",
        "        n_trials=num_samples,\n",
        "        resources_per_trial={\n",
        "            \"cpu\": 1,\n",
        "            \"gpu\": gpus_per_trial\n",
        "        },\n",
        "        scheduler=scheduler,\n",
        "        keep_checkpoints_num=1,\n",
        "        checkpoint_score_attr=\"training_iteration\",\n",
        "        stop=None,\n",
        "        progress_reporter=reporter,\n",
        "        local_dir=\"~/ray_results/\",\n",
        "        name=\"tune_transformer_pbt\",\n",
        "        log_to_file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-uiaCxNjrKw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "6e481eda-d103-4654-f4bf-c84987f31225"
      },
      "source": [
        "tune_transformer(encoded_train_dataset, encoded_test_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-1eaf18a266c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtune_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_test_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'encoded_train_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaMjFUzyYfNN"
      },
      "source": [
        "def model_init():\n",
        "    return AutoModelForSequenceClassification.from_pretrained(\n",
        "        'roberta-base', return_dict=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOoJymMpYVOH"
      },
      "source": [
        "trainer = Trainer(\n",
        "    args=args,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset= encoded_train_dataset, \n",
        "    eval_dataset=encoded_test_dataset,\n",
        "    model_init=model_init,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6voqTJ3gZCR"
      },
      "source": [
        "from ray.tune.schedulers import PopulationBasedTraining\n",
        "from ray.tune import uniform\n",
        "from random import randint\n",
        "from ray import tune\n",
        "\n",
        "scheduler = PopulationBasedTraining(\n",
        "    mode = \"max\",\n",
        "    metric='mean_accuracy',\n",
        "    perturbation_interval=2,\n",
        "    hyperparam_mutations={\n",
        "        \"weight_decay\": tune.uniform(0.0, 0.3),\n",
        "        \"learning_rate\": tune.uniform(1e-5, 5e-5),\n",
        "        \"per_device_train_batch_size\": tune.choice([16, 32, 64]),\n",
        "        \"num_train_epochs\": tune.choice([2,3,4]),\n",
        "        \"warmup_steps\":tune.choice(range(0, 500))\n",
        "    }\n",
        ")\n",
        "\n",
        "best_trial = trainer.hyperparameter_search(\n",
        "    direction=\"maximize\",\n",
        "    backend=\"ray\",\n",
        "    n_trials=10,\n",
        "    keep_checkpoints_num=1,\n",
        "    scheduler=scheduler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYg8q9XOlFfZ"
      },
      "source": [
        "best_run = trainer.hyperparameter_search(n_trials=10, direction=\"maximize\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-0epmcSaHC0"
      },
      "source": [
        "best_trial = trainer.hyperparameter_search(\n",
        "    direction=\"maximize\",\n",
        "    backend=\"ray\",\n",
        "    # Choose among many libraries:\n",
        "    # https://docs.ray.io/en/latest/tune/api_docs/suggestion.html\n",
        "    search_alg=HyperOptSearch(),\n",
        "    # Choose among schedulers:\n",
        "    # https://docs.ray.io/en/latest/tune/api_docs/schedulers.html\n",
        "    scheduler=AsyncHyperBand())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfvcxwkwY524"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}