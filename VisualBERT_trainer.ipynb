{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VisualBERT-trainer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i7JbvU1NC5j",
        "outputId": "e110cdbf-976e-42ad-a9be-5f60b236ed77"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGwLUaUgSL9n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d326a26-8eb5-4d4a-cfda-ff788e01ff74"
      },
      "source": [
        "! pip install transformers\n",
        "! pip install datasets \n",
        "! pip install --upgrade tqdm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 40.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 52.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 56.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.0\n",
            "Collecting datasets\n",
            "  Downloading datasets-1.10.2-py3-none-any.whl (542 kB)\n",
            "\u001b[K     |████████████████████████████████| 542 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.1)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 29.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting tqdm>=4.42\n",
            "  Downloading tqdm-4.61.2-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.12)\n",
            "Collecting fsspec>=2021.05.0\n",
            "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 38.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: tqdm, xxhash, fsspec, datasets\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed datasets-1.10.2 fsspec-2021.7.0 tqdm-4.61.2 xxhash-2.0.2\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.61.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMzk0W2H_3rB"
      },
      "source": [
        "!pip uninstall -y torch\n",
        "!pip install torch==1.7.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJ5PiuN_4SD4",
        "outputId": "ddfc8b70-484e-4fc8-ea18-1027e79f2da2"
      },
      "source": [
        "!pip install pytorch-lightning==1.3.8"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-lightning==1.3.8\n",
            "  Downloading pytorch_lightning-1.3.8-py3-none-any.whl (813 kB)\n",
            "\u001b[K     |████████████████████████████████| 813 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.8) (1.19.5)\n",
            "Collecting tensorboard!=2.5.0,>=2.2.0\n",
            "  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.6 MB 8.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.8) (1.9.0+cu102)\n",
            "Requirement already satisfied: PyYAML<=5.4.1,>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.8) (5.4.1)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.8) (2021.7.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.8) (21.0)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 51.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.8) (7.1.2)\n",
            "Collecting pyDeprecate==0.3.0\n",
            "  Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting torchmetrics>=0.2.0\n",
            "  Downloading torchmetrics-0.4.1-py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 54.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.8) (4.61.2)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 54.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning==1.3.8) (2.4.7)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (1.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (0.12.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (1.32.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (0.4.4)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (3.17.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (57.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (0.36.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (1.0.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (1.34.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch-lightning==1.3.8) (3.7.4.3)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 54.4 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 57.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8) (21.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8) (3.5.0)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=f22c8b8ee307744c12701bf85ea18547ca2d77bef03713456c62bc0cb4513c76\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: multidict, yarl, async-timeout, aiohttp, torchmetrics, tensorboard, pyDeprecate, future, pytorch-lightning\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.5.0 requires tensorboard~=2.5, but you have tensorboard 2.4.1 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 future-0.18.2 multidict-5.1.0 pyDeprecate-0.3.0 pytorch-lightning-1.3.8 tensorboard-2.4.1 torchmetrics-0.4.1 yarl-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy4zMdnsM3dw"
      },
      "source": [
        "!unzip -qq /content/drive/MyDrive/Hateful_Memes/hateful_memes.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsBkc_dbAcfh"
      },
      "source": [
        "import torch\n",
        "torch.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN8lH9xXSvs5"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cjLM5dPzzaV"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "dirpath = '/content/model-checkpoints'\n",
        "if os.path.exists(dirpath) and os.path.isdir(dirpath):\n",
        "    shutil.rmtree(dirpath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7nWls_fSzMi"
      },
      "source": [
        "df_train = pd.read_json('hateful_memes/train.jsonl', lines=True)\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQ1ODk-5OO5h"
      },
      "source": [
        "df_train.label.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSaxQs7nNYyK"
      },
      "source": [
        "val_seen = pd.read_json('hateful_memes/dev_seen.jsonl', lines=True)\n",
        "val_unseen = pd.read_json('hateful_memes/dev_unseen.jsonl', lines=True)\n",
        "df_val = pd.concat([val_seen, val_unseen],axis=0)\n",
        "df_val.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tTMcN4tNXAq"
      },
      "source": [
        "test_seen = pd.read_json('hateful_memes/test_seen.jsonl', lines=True)\n",
        "test_unseen = pd.read_json('hateful_memes/test_unseen.jsonl', lines=True)\n",
        "df_test = pd.concat([test_seen, test_unseen],axis=0)\n",
        "df_train.shape, df_val.shape, df_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNZioUGONoFk"
      },
      "source": [
        "df_val.label.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSDdEPzXN8rz"
      },
      "source": [
        "df_test.label.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCqW-BBgO1w5"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_192_dTP5Sn"
      },
      "source": [
        "\n",
        "df_train['text_len'] = df_train['text'].str.split().str.len()\n",
        "df_train['text_len'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmFf_MUKkAFD"
      },
      "source": [
        "df_train['idx'] = df_train['id'].astype(str).str.zfill(5)\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7aHKHGRkNG2"
      },
      "source": [
        "df_val['idx'] = df_val['id'].astype(str).str.zfill(5)\n",
        "df_test['idx'] = df_test['id'].astype(str).str.zfill(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M8zN1groWmC"
      },
      "source": [
        "df_train.shape, df_val.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKtw1cUxoM6b"
      },
      "source": [
        "## Remove records for which features couldn't be pulled correctly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqssbdPkSIXP"
      },
      "source": [
        "import pickle\n",
        "subset = False\n",
        "if subset:\n",
        "    with open('/content/drive/MyDrive/Hateful_Memes/features_100.pickle', 'rb') as handle:\n",
        "        features_dict = pickle.load(handle)\n",
        "else:\n",
        "    with open('/content/drive/MyDrive/Hateful_Memes/features.pickle', 'rb') as handle:\n",
        "        features_dict = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtqpDjFlqKCL"
      },
      "source": [
        "features_idx = list(features_dict.keys())\n",
        "train_idx = df_train['idx'].tolist()\n",
        "val_idx = df_val['idx'].tolist()\n",
        "print(len(features_idx), len(train_idx), len(val_idx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhbHx2GCqa6z"
      },
      "source": [
        "missing_train=[]\n",
        "for each in train_idx:\n",
        "    if each not in features_idx:\n",
        "        missing_train.append(each)\n",
        "\n",
        "\n",
        "missing_val=[]\n",
        "for each in val_idx:\n",
        "    if each not in features_idx:\n",
        "        missing_val.append(each)\n",
        "print(len(missing_train), len(missing_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-rMGI6ZoUkx"
      },
      "source": [
        "df_train = df_train[~df_train['idx'].isin(missing_train)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htLB2fMDoUoK"
      },
      "source": [
        "df_val = df_val[~df_val['idx'].isin(missing_val)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehMs4Is0olQh"
      },
      "source": [
        "df_train.shape, df_val.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_ieiNwrLUzt"
      },
      "source": [
        "## Compute Class Weight"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjGXoyRtLXuY"
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "y_train = df_train[\"label\"].values.tolist()\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                 np.unique(y_train),\n",
        "                                                 y_train)\n",
        "print(class_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgUHGzZFP4fG"
      },
      "source": [
        "df_train.label.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrmDPEtUe7Hm"
      },
      "source": [
        "## Load as a dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZawTj4oQ8Hw"
      },
      "source": [
        "subset = False\n",
        "if subset:\n",
        "    all_records = sorted(os.listdir('hateful_memes/img'))\n",
        "    selection = all_records[:100]\n",
        "    select_idx = [int(select.split('.')[0]) for select in selection]\n",
        "    df_train = df_train[df_train['id'].isin(select_idx)]\n",
        "    df_val = df_val[df_val['id'].isin(select_idx)]\n",
        "\n",
        "df_train.shape, df_val.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15HVpWiKPSkH"
      },
      "source": [
        "from datasets import list_metrics, load_metric\n",
        "metrics_list = list_metrics()\n",
        "print(metrics_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kv4m5UAf1B3"
      },
      "source": [
        "acc_metric = load_metric('accuracy')\n",
        "f1_metric = load_metric('f1')\n",
        "precision_metric = load_metric('precision')\n",
        "recall_metric = load_metric('recall')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZD53AeagF4D"
      },
      "source": [
        "## Create Dataset function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olZeIJOhgDty"
      },
      "source": [
        "from transformers import BertTokenizer, VisualBertForPreTraining, AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ0oB82ISJHA"
      },
      "source": [
        "## Load Visual Embedding features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeeChXlYk1QF"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pIK2KYvSnh-"
      },
      "source": [
        "class HatefulMemesData(Dataset):\n",
        "    def __init__(self, df, tokenizer, sequence_length, \n",
        "                 print_text=False):         \n",
        "\n",
        "        self.sequence_length = sequence_length\n",
        "        self.tokenizer = tokenizer\n",
        "        self.print_text = print_text\n",
        "\n",
        "        texts = df[\"text\"].values.tolist()\n",
        "        labels = df[\"label\"].values.tolist()\n",
        "        ids =  df[\"idx\"].values.tolist()\n",
        "\n",
        "        self.dataset = []\n",
        "        for i, inp in enumerate(texts):\n",
        "            self.dataset.append({\"text\": inp, \"label\": labels[i], 'idx': ids[i]})\n",
        "  \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "\n",
        "    def tokenize_data(self, example):\n",
        "   \n",
        "        idx = example['idx']\n",
        "        idx = [idx] if isinstance(idx, str) else idx\n",
        "        # encoded_dict = tokenizer.batch_encode_plus(example['text'], padding='max_length', max_length=max_len, truncation=True, return_tensors='pt')\n",
        "        encoded_dict = tokenizer(example['text'], padding='max_length', max_length=self.sequence_length, truncation=True, return_tensors='pt')\n",
        "        tokens = encoded_dict['input_ids']\n",
        "        token_type_ids = encoded_dict['token_type_ids']\n",
        "        attn_mask = encoded_dict['attention_mask']\n",
        "        \n",
        "        targets = torch.tensor(example['label']).type(torch.int64)\n",
        "        embed_list = [features_dict[idval] for idval in idx]\n",
        "        embed_list = np.array(embed_list)\n",
        "        visual_embeds = torch.from_numpy(embed_list).double()\n",
        "        # visual_embeds = visual_embeds.repeat(1,1,2)\n",
        "\n",
        "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
        "        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
        "\n",
        "        inputs={\"input_ids\": tokens.squeeze(),\n",
        "            \"attention_mask\": attn_mask.squeeze(),\n",
        "            \"token_type_ids\": token_type_ids.squeeze(),\n",
        "            \"visual_embeds\": visual_embeds.squeeze(),\n",
        "            \"visual_token_type_ids\": visual_token_type_ids.squeeze(),\n",
        "            \"visual_attention_mask\": visual_attention_mask.squeeze(),\n",
        "            \"label\": targets.squeeze()\n",
        "        }\n",
        "        \n",
        "        return inputs\n",
        "  \n",
        "    def __getitem__(self, index):\n",
        "        inputs = self.tokenize_data(self.dataset[index])\n",
        "        \n",
        "        if self.print_text:\n",
        "            for k in inputs.keys():\n",
        "                print(k, inputs[k].shape, inputs[k].dtype)\n",
        "\n",
        "        return inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5W2mYO_kLLf"
      },
      "source": [
        "dataset = HatefulMemesData(df_val, tokenizer, 50, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lz3WbW5kUog"
      },
      "source": [
        "example1 = dataset[5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LTa5sqUkI_J"
      },
      "source": [
        "## Fine-Tune Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdLo_MtAkCZt"
      },
      "source": [
        "from transformers import BertTokenizer, VisualBertModel, TrainingArguments, Trainer\n",
        "\n",
        "model = VisualBertModel.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWcMWWgzF71h"
      },
      "source": [
        "# example1 = tokenize_data(df_train.to_dict('records')[0])\n",
        "print(example1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ou29gT0lgbv2"
      },
      "source": [
        "example1['input_ids'].unsqueeze(0).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXGtuj6f2XgR"
      },
      "source": [
        "model = model.double()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_1lUf09D91Z"
      },
      "source": [
        "outputs = model(input_ids=example1['input_ids'].unsqueeze(0),\n",
        "                attention_mask=example1['attention_mask'].unsqueeze(0),\n",
        "                visual_token_type_ids=example1['visual_token_type_ids'].unsqueeze(0),\n",
        "                token_type_ids=example1['token_type_ids'].unsqueeze(0),\n",
        "                visual_embeds=example1['visual_embeds'].unsqueeze(0),\n",
        "                visual_attention_mask=example1['visual_attention_mask'].unsqueeze(0),\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67hOqJMSK7GX"
      },
      "source": [
        "pooled_outputs = outputs[1]\n",
        "print(pooled_outputs.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAhhNY4y4MUa"
      },
      "source": [
        "## Tuning using Pytorch Lightning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8Nh2K8j4L1g"
      },
      "source": [
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from datasets import load_metric\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    VisualBertModel,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "import logging\n",
        "import argparse\n",
        "import time\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpsDvM-r4L4T"
      },
      "source": [
        "# from pytorch_lightning.loggers.wandb import WandbLogger\n",
        "import os\n",
        "from pathlib import Path\n",
        "from string import punctuation\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-hl24bz3obU"
      },
      "source": [
        "## Look at Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-_UouKgObfk"
      },
      "source": [
        "weights = [0.77510622, 1.40873991]\n",
        "wt_tensor = torch.FloatTensor(weights).cuda()\n",
        "print(wt_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTSuZE5G3FNu"
      },
      "source": [
        "class VisualBERTClassifier(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
        "        member variables.\n",
        "        \"\"\"\n",
        "        super(VisualBERTClassifier, self).__init__()\n",
        "        self.visualbert = VisualBertModel.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre')\n",
        "        self.num_labels = 2\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.cls=  nn.Linear(768, self.num_labels)\n",
        "        self.weight = torch.FloatTensor([0.77510622, 1.40873991]),\n",
        "\n",
        "        nSamples = [5178, 2849]\n",
        "        normedWeights = [1 - (x / sum(nSamples)) for x in nSamples]\n",
        "        self.loss_fct = CrossEntropyLoss(weight=torch.FloatTensor(normedWeights))\n",
        "        \n",
        "    \n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, visual_embeds, visual_attention_mask,\n",
        "                visual_token_type_ids, labels):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Tensor of input data and we must return\n",
        "        a Tensor of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Tensors.\n",
        "        \"\"\"\n",
        "        outputs = self.visualbert(\n",
        "                input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids,\n",
        "                visual_embeds=visual_embeds,\n",
        "                visual_attention_mask=visual_attention_mask,\n",
        "                visual_token_type_ids=visual_token_type_ids,\n",
        "            )\n",
        "        \n",
        "        pooled_output = outputs[1]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.cls(pooled_output)\n",
        "        reshaped_logits = logits.view(-1, self.num_labels)\n",
        "\n",
        "        loss = self.loss_fct(reshaped_logits, labels.view(-1))\n",
        "      \n",
        "        return loss, reshaped_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6v7whCY5Qwy"
      },
      "source": [
        "model = VisualBERTClassifier().to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnw7jNeFyIEc"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7tblaBP7Gb_"
      },
      "source": [
        "## Using HuggingFace Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q64H3ECW7Jq_"
      },
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "batch_size = 48\n",
        "seq_len = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRuUBtBe8JYR"
      },
      "source": [
        "model = VisualBERTClassifier()\n",
        "model = model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UefymyHr7Jt_"
      },
      "source": [
        "metric_name = \"accuracy\"\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir = \"model-checkpoint\",\n",
        "    seed = 110, \n",
        "    evaluation_strategy = \"steps\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=40,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_name,\n",
        "    eval_steps = 500,\n",
        "    save_steps = 500,\n",
        "    fp16 = False,\n",
        "    gradient_accumulation_steps = 2\n",
        "\n",
        "\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-C8d6fw7Jwh"
      },
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
        "    auc_score = roc_auc_score(labels, predictions)\n",
        "    return {\"accuracy\": acc['accuracy'], \"auroc\": auc_score} "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwSU1fNp70mt"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset = HatefulMemesData(df_train,tokenizer=tokenizer, sequence_length=seq_len),\n",
        "    eval_dataset =  HatefulMemesData(df_val,tokenizer=tokenizer, sequence_length=seq_len),\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c7lrnLe9lUO"
      },
      "source": [
        "## To resume from an old checkpoint, set path in resume-from\n",
        "resume_from ='/content/model-checkpoint/checkpoint-12000'\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIzbDwJeY08R"
      },
      "source": [
        "import numpy as np\n",
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pm3hl5JY3k8"
      },
      "source": [
        "trainer.save_model('VisualBERT_classification_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G58y74pJY7uK"
      },
      "source": [
        "!zip -r 'VisualBERT_classification_model.zip' 'VisualBERT_classification_model'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O41GPWcFd3mJ"
      },
      "source": [
        "!mv VisualBERT_classification_model.zip.zip /content/drive/MyDrive/Hateful_Memes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hJiCXVyYn99"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPXnT1KPYo6a"
      },
      "source": [
        "## Pytorch Lightning version of code - May have bugs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Vbp3YOQ4L69"
      },
      "source": [
        "class VisualBERTFineTuner(pl.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super(VisualBERTFineTuner, self).__init__()      \n",
        "        self.model = VisualBERTClassifier().double()\n",
        "        self.num_labels = 2\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(768, self.num_labels)\n",
        "\n",
        "        self.problem_type = 'single_label_classification'\n",
        "        self.save_hyperparameters(hparams)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.output_dir = Path(self.hparams.output_dir)\n",
        "        self.total_steps = 0\n",
        "\n",
        "    def is_logger(self):\n",
        "        return self.trainer.global_rank <= 0\n",
        "    \n",
        "        \n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, visual_embeds, visual_attention_mask,\n",
        "                visual_token_type_ids, labels):\n",
        "        loss, preds = self.model(\n",
        "                input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids,\n",
        "                visual_embeds=visual_embeds,\n",
        "                visual_attention_mask=visual_attention_mask,\n",
        "                visual_token_type_ids=visual_token_type_ids,\n",
        "                labels = labels\n",
        "            )\n",
        "        return loss, preds \n",
        "\n",
        "    # def loss(self, batch, prediction):\n",
        "    #     loss_fct = CrossEntropyLoss()\n",
        "    #     labels = batch['label']\n",
        "    #     loss = loss_fct(prediction.view(-1, self.num_labels), labels.view(-1))\n",
        "    #     return loss\n",
        "   \n",
        "\n",
        "    def _step(self, batch):\n",
        "        outputs = self(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"],\n",
        "            token_type_ids=batch[\"token_type_ids\"],\n",
        "            visual_embeds=batch[\"visual_embeds\"],\n",
        "            visual_attention_mask=batch[\"visual_attention_mask\"],\n",
        "            visual_token_type_ids=batch[\"visual_token_type_ids\"],\n",
        "            labels = batch['label']\n",
        "        )\n",
        "\n",
        "        return outputs\n",
        "    \n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, preds = self._step(batch)\n",
        "        return loss\n",
        "  \n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        val_loss, preds = self._step(batch)\n",
        "        preds = torch.argmax(preds, axis=1)\n",
        "        labels = batch[\"label\"]\n",
        "        return {'loss': val_loss, \"preds\": preds, \"labels\": labels}\n",
        "\n",
        "    \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        preds = torch.cat([x['preds'] for x in outputs]).detach().cpu().numpy()\n",
        "        labels = torch.cat([x['labels'] for x in outputs]).detach().cpu().numpy()\n",
        "        loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
        "        auc_score = roc_auc_score(labels, preds, average='weighted')\n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "        self.log('val_auroc', auc_score, prog_bar=True)\n",
        "        self.log_dict(acc_metric.compute(predictions=preds, references=labels), prog_bar=True)\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "\n",
        "        model = self.model\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": self.hparams.weight_decay,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
        "\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=self.total_steps\n",
        "        )\n",
        "        scheduler = {'scheduler': scheduler, 'interval': 'step', 'frequency': 1}\n",
        "        return [optimizer], [scheduler]\n",
        "  \n",
        "    # def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx,\n",
        "    #                    closure, on_tpu=False, using_native_amp=False, using_lbfgs=False):\n",
        "    #     model = self.model\n",
        "    #     if self.trainer.use_tpu:\n",
        "    #         xm.optimizer_step(optimizer)\n",
        "    #     else:\n",
        "    #         optimizer.step(closure=closure)\n",
        "    #     optimizer.zero_grad()\n",
        "    #     torch.nn.utils.clip_grad_norm_(model.parameters(), self.hparams.max_grad_norm)\n",
        "    #     self.lr_scheduler.step()\n",
        "  \n",
        "    def get_tqdm_dict(self):\n",
        "        tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
        "\n",
        "        return tqdm_dict\n",
        "    \n",
        "    def train_dataloader(self):   \n",
        "        train_loader = DataLoader(HatefulMemesData(df_train, self.tokenizer, self.hparams.max_input_length),\n",
        "                                batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True, num_workers=2)\n",
        "       # Calculate total steps\n",
        "        tb_size = self.hparams.train_batch_size * max(1, self.trainer.gpus)\n",
        "        ab_size = self.trainer.accumulate_grad_batches * float(self.trainer.max_epochs)\n",
        "        self.total_steps = (len(train_loader.dataset) // tb_size) // ab_size\n",
        "\n",
        "        return train_loader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(HatefulMemesData(df_val, self.tokenizer, self.hparams.max_input_length),\n",
        "                          batch_size=self.hparams.eval_batch_size, num_workers=2)\n",
        "    \n",
        "    def test_dataloader(self):\n",
        "        return  DataLoader(HatefulMemesData(df_test, self.tokenizer, self.hparams.max_input_length),\n",
        "                          batch_size=self.hparams.eval_batch_size, num_workers=2)\n",
        "    \n",
        "    def on_save_checkpoint(self, checkpoint):\n",
        "        save_path = self.output_dir.joinpath(model_prefix)\n",
        "        self.model.config.save_step = self.step_count\n",
        "        self.model.save_pretrained(save_path)\n",
        "        self.tokenizer.save_pretrained(save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2H1WD8L-KZw"
      },
      "source": [
        "# from pytorch_lightning import loggers as pl_loggers\n",
        "# tb_logger = pl_loggers.TensorBoardLogger('logs/')\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "class LoggingCallback(pl.Callback):\n",
        "  def on_validation_end(self, trainer, pl_module):\n",
        "    logger.info(\"***** Validation results *****\")\n",
        "    if pl_module.is_logger():\n",
        "      metrics = trainer.callback_metrics\n",
        "      # Log results\n",
        "      for key in sorted(metrics):\n",
        "        if key not in [\"log\", \"progress_bar\"]:\n",
        "          logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "\n",
        "  def on_test_end(self, trainer, pl_module):\n",
        "    logger.info(\"***** Test results *****\")\n",
        "\n",
        "    if pl_module.is_logger():\n",
        "      metrics = trainer.callback_metrics\n",
        "\n",
        "      # Log and save results to file\n",
        "      output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n",
        "      with open(output_test_results_file, \"w\") as writer:\n",
        "        for key in sorted(metrics):\n",
        "          if key not in [\"log\", \"progress_bar\"]:\n",
        "            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnCCYhNV-7Cy"
      },
      "source": [
        "model_name = \"visualbert\"\n",
        "token_len = 50\n",
        "model_prefix = f\"{model_name}-{token_len}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2rdAUs0-Pbj"
      },
      "source": [
        "args_dict = dict(\n",
        "    output_dir=\"\", # path to save the checkpoints\n",
        "    max_input_length=token_len,\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=0.0,\n",
        "    adam_epsilon=1e-8,\n",
        "    warmup_steps=0,\n",
        "    train_batch_size=4,\n",
        "    eval_batch_size=4,\n",
        "    num_train_epochs=5,\n",
        "    gradient_accumulation_steps=1,\n",
        "    n_gpu=1,\n",
        "    resume_from_checkpoint=None, \n",
        "    val_check_interval = 0.5, \n",
        "    early_stop_callback=False,\n",
        "    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
        "    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
        "    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "\n",
        "args_dict.update({'output_dir': \"./\" + model_prefix + \"_final\", 'num_train_epochs':6,\n",
        "             'train_batch_size': 32, 'eval_batch_size': 32})\n",
        "args = argparse.Namespace(**args_dict)\n",
        "\n",
        "\n",
        "## Define Checkpoint function\n",
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "    dirpath=\"./\" + model_prefix + \"_checkpoint\", filename=model_prefix, monitor=\"accuracy\", mode=\"max\", save_top_k=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH5bESY5vVoe"
      },
      "source": [
        "print(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTjBlV4l-q_N"
      },
      "source": [
        "train_params = dict(\n",
        "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
        "    gpus=min(1, torch.cuda.device_count()),\n",
        "    max_epochs=args.num_train_epochs,\n",
        "    precision= 16 if args.fp_16 else 32,\n",
        "    amp_level=args.opt_level,\n",
        "    resume_from_checkpoint=args.resume_from_checkpoint,\n",
        "    # gradient_clip_val=args.max_grad_norm,\n",
        "    checkpoint_callback=checkpoint_callback,\n",
        "    val_check_interval=args.val_check_interval,\n",
        "    callbacks=[LoggingCallback()],\n",
        "    # logger=tb_logger\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwmhgyIB-r5e"
      },
      "source": [
        "model = VisualBERTFineTuner(args)\n",
        "trainer = pl.Trainer(**train_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1x3bI6Sw6Rb"
      },
      "source": [
        "trainer.fit(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAhPxMZx6kg3"
      },
      "source": [
        "## Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOB-4LmLasvz"
      },
      "source": [
        "! pip install optuna -q\n",
        "! pip install 'ray[tune]'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehKa09yWRBJE"
      },
      "source": [
        "import ray\n",
        "ray.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a8IrKkuZRy7"
      },
      "source": [
        "! pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci74nEW2iYLO"
      },
      "source": [
        "import os\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import PopulationBasedTraining\n",
        "from transformers import  AutoConfig, \\\n",
        "    AutoModelForSequenceClassification, AutoTokenizer, Trainer, \\\n",
        "     TrainingArguments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vsUnjoaklxn"
      },
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        " \n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
        "    # precision = precision_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
        "    # recall = recall_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeCfBcgqigqi"
      },
      "source": [
        "def tune_transformer( train_dataset,\n",
        "                     test_dataset,\n",
        "                    num_samples=8,\n",
        "                     gpus_per_trial=0,\n",
        "                     num_labels=5,\n",
        "                     ray_address=None):\n",
        "    \n",
        "    #ray.shutdown()\n",
        "    #ray.init(ray_address, log_to_driver=False)\n",
        "    data_dir_name = \"./data\" \n",
        "    data_dir = os.path.abspath(os.path.join(os.getcwd(), data_dir_name))\n",
        "    if not os.path.exists(data_dir):\n",
        "        os.mkdir(data_dir, 0o755)\n",
        "\n",
        "    # Change these as needed.\n",
        "    model_name = \"roberta-base\" \n",
        "\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        model_name, num_labels=num_labels )\n",
        "\n",
        "    # Download and cache tokenizer, model, and features\n",
        "    print(\"Downloading and caching Tokenizer\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Triggers tokenizer download to cache\n",
        "    print(\"Downloading and caching pre-trained model\")\n",
        "    AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        config=config,\n",
        "    )\n",
        "\n",
        "    def get_model():\n",
        "        return AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            config=config,\n",
        "        )\n",
        "\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\".\",\n",
        "        learning_rate=1e-5,  # config\n",
        "        do_train=True,\n",
        "        do_eval=True,\n",
        "        no_cuda=gpus_per_trial <= 0,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        num_train_epochs=2,  # config\n",
        "        max_steps=-1,\n",
        "        per_device_train_batch_size=16,  # config\n",
        "        per_device_eval_batch_size=16,  # config\n",
        "        warmup_steps=0,\n",
        "        weight_decay=0.1,  # config\n",
        "        logging_dir=\"./logs\",\n",
        "    )\n",
        "\n",
        "    training_args._n_gpu = gpus_per_trial\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model_init=get_model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        compute_metrics=compute_metrics)\n",
        "\n",
        "    tune_config = {\n",
        "        \"per_device_train_batch_size\": 32,\n",
        "        \"per_device_eval_batch_size\": 32,\n",
        "        \"num_train_epochs\": tune.choice([2, 3, 4, 5]),\n",
        "        \"max_steps\": 1 \n",
        "    }\n",
        "\n",
        "    scheduler = PopulationBasedTraining(\n",
        "        time_attr=\"training_iteration\",\n",
        "        metric=\"eval_acc\",\n",
        "        mode=\"max\",\n",
        "        perturbation_interval=1,\n",
        "        hyperparam_mutations={\n",
        "            \"weight_decay\": tune.uniform(0.0, 0.3),\n",
        "            \"learning_rate\": tune.uniform(1e-5, 5e-5),\n",
        "            \"per_device_train_batch_size\": [16, 32, 64],\n",
        "        })\n",
        "\n",
        "    reporter = CLIReporter(\n",
        "        parameter_columns={\n",
        "            \"weight_decay\": \"w_decay\",\n",
        "            \"learning_rate\": \"lr\",\n",
        "            \"per_device_train_batch_size\": \"train_bs/gpu\",\n",
        "            \"num_train_epochs\": \"num_epochs\"\n",
        "        },\n",
        "        metric_columns=[\n",
        "            \"eval_acc\", \"eval_loss\", \"epoch\", \"training_iteration\"\n",
        "        ])\n",
        "\n",
        "    trainer.hyperparameter_search(\n",
        "        hp_space=lambda _: tune_config,\n",
        "        backend=\"ray\",\n",
        "        n_trials=num_samples,\n",
        "        resources_per_trial={\n",
        "            \"cpu\": 1,\n",
        "            \"gpu\": gpus_per_trial\n",
        "        },\n",
        "        scheduler=scheduler,\n",
        "        keep_checkpoints_num=1,\n",
        "        checkpoint_score_attr=\"training_iteration\",\n",
        "        stop=None,\n",
        "        progress_reporter=reporter,\n",
        "        local_dir=\"~/ray_results/\",\n",
        "        name=\"tune_transformer_pbt\",\n",
        "        log_to_file=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-uiaCxNjrKw"
      },
      "source": [
        "tune_transformer(encoded_train_dataset, encoded_test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaMjFUzyYfNN"
      },
      "source": [
        "def model_init():\n",
        "    return AutoModelForSequenceClassification.from_pretrained(\n",
        "        'roberta-base', return_dict=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOoJymMpYVOH"
      },
      "source": [
        "trainer = Trainer(\n",
        "    args=args,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset= encoded_train_dataset, \n",
        "    eval_dataset=encoded_test_dataset,\n",
        "    model_init=model_init,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6voqTJ3gZCR"
      },
      "source": [
        "from ray.tune.schedulers import PopulationBasedTraining\n",
        "from ray.tune import uniform\n",
        "from random import randint\n",
        "from ray import tune\n",
        "\n",
        "scheduler = PopulationBasedTraining(\n",
        "    mode = \"max\",\n",
        "    metric='mean_accuracy',\n",
        "    perturbation_interval=2,\n",
        "    hyperparam_mutations={\n",
        "        \"weight_decay\": tune.uniform(0.0, 0.3),\n",
        "        \"learning_rate\": tune.uniform(1e-5, 5e-5),\n",
        "        \"per_device_train_batch_size\": tune.choice([16, 32, 64]),\n",
        "        \"num_train_epochs\": tune.choice([2,3,4]),\n",
        "        \"warmup_steps\":tune.choice(range(0, 500))\n",
        "    }\n",
        ")\n",
        "\n",
        "best_trial = trainer.hyperparameter_search(\n",
        "    direction=\"maximize\",\n",
        "    backend=\"ray\",\n",
        "    n_trials=10,\n",
        "    keep_checkpoints_num=1,\n",
        "    scheduler=scheduler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYg8q9XOlFfZ"
      },
      "source": [
        "best_run = trainer.hyperparameter_search(n_trials=10, direction=\"maximize\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-0epmcSaHC0"
      },
      "source": [
        "best_trial = trainer.hyperparameter_search(\n",
        "    direction=\"maximize\",\n",
        "    backend=\"ray\",\n",
        "    # Choose among many libraries:\n",
        "    # https://docs.ray.io/en/latest/tune/api_docs/suggestion.html\n",
        "    search_alg=HyperOptSearch(),\n",
        "    # Choose among schedulers:\n",
        "    # https://docs.ray.io/en/latest/tune/api_docs/schedulers.html\n",
        "    scheduler=AsyncHyperBand())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfvcxwkwY524"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}